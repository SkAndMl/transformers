{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import Multi30k\n",
    "\n",
    "import json\n",
    "\n",
    "from typing import Dict, Tuple, List\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from model import TransformerForSeqToSeq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 64,\n",
       " 'block_size': 64,\n",
       " 'context_length': 512,\n",
       " 'd_model': 384,\n",
       " 'dropout': 0.1,\n",
       " 'head_dim': 64,\n",
       " 'learning_rate': 0.0003,\n",
       " 'n_decoders': 6,\n",
       " 'n_encoders': 6,\n",
       " 'n_heads': 6,\n",
       " 'train_iters': 10}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../config/base_config.json\") as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "PAD_TOKEN = 0\n",
    "UNK_TOKEN = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = Multi30k(split=\"train\")\n",
    "val_iter = Multi30k(split=\"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = get_tokenizer(tokenizer=\"spacy\", language=\"de_core_news_sm\")\n",
    "tgt_tokenizer = get_tokenizer(tokenizer=\"spacy\", language=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sathyakrishnansuresh/miniforge3/envs/tf/lib/python3.11/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8014, 6191)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_vocab(tokenizer, idx, data_iter):\n",
    "\n",
    "    for instance in data_iter:\n",
    "        yield tokenizer(instance[idx])\n",
    "\n",
    "src_vocab = build_vocab_from_iterator(build_vocab(src_tokenizer, 0, train_iter),\n",
    "                                      min_freq=2,\n",
    "                                      special_first=True,\n",
    "                                      specials=[\"[PAD]\", \"[SOS]\", \"[EOS]\", \"[UNK]\"])\n",
    "tgt_vocab = build_vocab_from_iterator(build_vocab(tgt_tokenizer, 1, train_iter),\n",
    "                                      min_freq=2,\n",
    "                                      special_first=True,\n",
    "                                      specials=[\"[PAD]\", \"[SOS]\", \"[EOS]\", \"[UNK]\"])\n",
    "\n",
    "src_vocab.set_default_index(UNK_TOKEN)\n",
    "tgt_vocab.set_default_index(UNK_TOKEN)\n",
    "\n",
    "len(src_vocab), len(tgt_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1,   3,  15,   3, 601,   2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt = \"Hey, hi there\"\n",
    "tgt_tokens = tgt_tokenizer(tgt)\n",
    "tgt_ids = tgt_vocab(tgt_tokens)\n",
    "torch.cat([torch.tensor([SOS_TOKEN]), torch.tensor(tgt_ids), torch.tensor([EOS_TOKEN])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    src_batch, tgt_batch = [], []\n",
    "\n",
    "    for src, tgt in batch:\n",
    "        src = src_vocab(src_tokenizer(src.rstrip(\"\\n\")))\n",
    "        tgt = tgt_vocab(tgt_tokenizer(tgt.rstrip(\"\\n\")))\n",
    "        \n",
    "        src = torch.cat([torch.tensor([SOS_TOKEN]), torch.tensor(src), torch.tensor([EOS_TOKEN])])\n",
    "        tgt = torch.cat([torch.tensor([SOS_TOKEN]), torch.tensor(tgt), torch.tensor([EOS_TOKEN])])\n",
    "        \n",
    "        src_batch.append(src)\n",
    "        tgt_batch.append(tgt)\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, batch_first=True, padding_value=PAD_TOKEN)\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_TOKEN)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset=train_iter,\n",
    "                              batch_size=config[\"batch_size\"],\n",
    "                              collate_fn=collate_fn,\n",
    "                              shuffle=True)\n",
    "\n",
    "val_dataloader = DataLoader(dataset=val_iter,\n",
    "                            batch_size=config[\"batch_size\"],\n",
    "                            collate_fn=collate_fn,\n",
    "                            shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerForSeqToSeq(config=config, src_vocab_size=len(src_vocab),\n",
    "                                     tgt_vocab_size=len(tgt_vocab), padding_idx=PAD_TOKEN).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "TransformerForSeqToSeq                                  --\n",
       "├─Embedding: 1-1                                        --\n",
       "│    └─Embedding: 2-1                                   3,077,376\n",
       "│    └─Embedding: 2-2                                   196,608\n",
       "│    └─Dropout: 2-3                                     --\n",
       "├─Embedding: 1-2                                        --\n",
       "│    └─Embedding: 2-4                                   2,377,344\n",
       "│    └─Embedding: 2-5                                   196,608\n",
       "│    └─Dropout: 2-6                                     --\n",
       "├─Encoder: 1-3                                          --\n",
       "│    └─ModuleList: 2-7                                  --\n",
       "│    │    └─EncoderBlock: 3-1                           1,774,464\n",
       "│    │    └─EncoderBlock: 3-2                           1,774,464\n",
       "│    │    └─EncoderBlock: 3-3                           1,774,464\n",
       "│    │    └─EncoderBlock: 3-4                           1,774,464\n",
       "│    │    └─EncoderBlock: 3-5                           1,774,464\n",
       "│    │    └─EncoderBlock: 3-6                           1,774,464\n",
       "├─Decoder: 1-4                                          --\n",
       "│    └─ModuleList: 2-8                                  --\n",
       "│    │    └─DecoderBlock: 3-7                           2,366,592\n",
       "│    │    └─DecoderBlock: 3-8                           2,366,592\n",
       "│    │    └─DecoderBlock: 3-9                           2,366,592\n",
       "│    │    └─DecoderBlock: 3-10                          2,366,592\n",
       "│    │    └─DecoderBlock: 3-11                          2,366,592\n",
       "│    │    └─DecoderBlock: 3-12                          2,366,592\n",
       "├─Sequential: 1-5                                       --\n",
       "│    └─Dropout: 2-9                                     --\n",
       "│    └─Linear: 2-10                                     2,383,535\n",
       "================================================================================\n",
       "Total params: 33,077,807\n",
       "Trainable params: 33,077,807\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(transformer: TransformerForSeqToSeq,\n",
    "               dataloader: DataLoader,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               device: torch.device=\"cpu\") -> float:\n",
    "    \n",
    "    transformer.train()\n",
    "    losses = 0\n",
    "    for batch, (src_batch, tgt_batch) in enumerate(dataloader):\n",
    "\n",
    "        src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
    "        logits, loss = transformer(src_batch, tgt_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses += loss.item()\n",
    "    \n",
    "    return losses/len(dataloader)\n",
    "\n",
    "def eval_step(transformer: TransformerForSeqToSeq,\n",
    "              dataloader: DataLoader,\n",
    "              device: torch.device=\"cpu\") -> float:\n",
    "    \n",
    "    transformer.eval()\n",
    "    losses = 0\n",
    "    with torch.inference_mode():\n",
    "\n",
    "        for batch, (src_batch, tgt_batch) in enumerate(dataloader):\n",
    "            src_batch, tgt_batch = src_batch.to(device), tgt_batch.to(device)\n",
    "            logits, loss = transformer(src_batch, tgt_batch)\n",
    "\n",
    "            losses += loss.item()\n",
    "        losses /= len(dataloader)\n",
    "    \n",
    "    return losses\n",
    "\n",
    "\n",
    "def train(transformer: TransformerForSeqToSeq,\n",
    "          train_dataloader: DataLoader,\n",
    "          val_dataloader: DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          config: Dict[str, int],\n",
    "          device: torch.device=\"cpu\") -> Tuple[List[float], List[float]]:\n",
    "    \n",
    "    train_losses, eval_losses = [], []\n",
    "\n",
    "    transformer.to(device=device)\n",
    "\n",
    "    for epoch in range(1, config[\"train_iters\"]+1):\n",
    "        \n",
    "        train_loss = train_step(transformer, train_dataloader, optimizer, device)\n",
    "        val_loss = eval_step(transformer, val_dataloader, device)\n",
    "\n",
    "        print(f\"epoch {epoch}: train_loss: {train_loss:.4f} val_loss: {val_loss: .4f}\")\n",
    "        train_losses.append(train_loss)\n",
    "        eval_losses.append(val_loss)\n",
    "    \n",
    "    return train_losses, eval_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TransformerForSeqToSeq(config=config,\n",
    "                                     src_vocab_size=len(src_vocab),\n",
    "                                     tgt_vocab_size=len(tgt_vocab),\n",
    "                                     padding_idx=PAD_TOKEN)\n",
    "optimizer = torch.optim.AdamW(params=transformer.parameters(),\n",
    "                              lr=config[\"learning_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, eval_losses = train(transformer=transformer,\n",
    "                                  train_dataloader=train_dataloader,\n",
    "                                  val_dataloader=val_dataloader,\n",
    "                                  optimizer=optimizer,\n",
    "                                  config=config,\n",
    "                                  device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
