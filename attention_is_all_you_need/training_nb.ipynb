{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchtext\n",
    "import numpy as np\n",
    "from typing import Optional, List, Dict, Tuple, Iterable\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = torchtext.datasets.Multi30k(split=\"train\")\n",
    "test_iter = torchtext.datasets.Multi30k(split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tokenizer = torchtext.data.utils.get_tokenizer(\"spacy\", language=\"de_core_news_sm\")\n",
    "tgt_tokenizer = torchtext.data.utils.get_tokenizer(\"spacy\", language=\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi', 'there']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_tokenizer(\"Hi there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sathyakrishnansuresh/miniforge3/envs/tf/lib/python3.11/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    }
   ],
   "source": [
    "PAD_IDX, UNK_IDX, SOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "specials = ['<unk>', '<pad>', '<sos>', '<eos>']\n",
    "\n",
    "def build_vocab(data_iter, tokenizer, lang_idx):\n",
    "    for instance in data_iter:\n",
    "        yield tokenizer(instance[lang_idx])\n",
    "\n",
    "\n",
    "src_vocab = torchtext.vocab.build_vocab_from_iterator(build_vocab(train_iter, src_tokenizer, 0),\n",
    "                                                      min_freq=1,\n",
    "                                                      special_first=True,\n",
    "                                                      specials=specials)\n",
    "tgt_vocab = torchtext.vocab.build_vocab_from_iterator(build_vocab(train_iter, tgt_tokenizer, 1),\n",
    "                                                      min_freq=1,\n",
    "                                                      special_first=True,\n",
    "                                                      specials=specials)\n",
    "\n",
    "src_vocab.set_default_index(UNK_IDX)\n",
    "tgt_vocab.set_default_index(UNK_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,    1, 8920,   15,  889,   17, 1328, 2470,    3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"Hey hi, how are you?\"\n",
    "a_tok = tgt_tokenizer(a)\n",
    "a_voc = tgt_vocab(a_tok)\n",
    "torch.cat((torch.tensor([SOS_IDX]), torch.tensor(a_voc), torch.tensor([EOS_IDX])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(sentence: str, lang_idx: int):\n",
    "    if lang_idx == 0:\n",
    "        tokens = src_tokenizer(sentence)\n",
    "        token_ids = src_vocab(tokens)\n",
    "        return torch.cat((torch.tensor([SOS_IDX]),\n",
    "                          torch.tensor(token_ids),\n",
    "                          torch.tensor([EOS_IDX])))\n",
    "    else:\n",
    "        tokens = tgt_tokenizer(sentence)\n",
    "        token_ids = tgt_vocab(tokens)\n",
    "        return torch.cat((torch.tensor([SOS_IDX]),\n",
    "                          torch.tensor(token_ids),\n",
    "                          torch.tensor([EOS_IDX])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   2,    1, 8920,   15,  889,   17, 1328, 2470,    3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2],\n",
       "        [0, 1, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [\n",
    "    torch.tensor([0, 1, 2]),\n",
    "    torch.tensor([0, 1])    \n",
    "]\n",
    "pad_sequence(a, padding_value=PAD_IDX, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src, tgt in batch:\n",
    "        src_batch.append(transform(src.rstrip(\"\\n\"), 0))\n",
    "        tgt_batch.append(transform(tgt.rstrip(\"\\n\"), 1))\n",
    "    \n",
    "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, batch_first=True, padding_value=PAD_IDX)\n",
    "    return src_batch, tgt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_iter, \n",
    "                                               batch_size=32,\n",
    "                                               collate_fn=collate_fn)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_iter,\n",
    "                                              batch_size=32,\n",
    "                                              collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 21])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 43)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len_src, max_seq_len_tgt = 0, 0\n",
    "\n",
    "for src_batch, tgt_batch in train_dataloader:\n",
    "    max_seq_len_src = max(max_seq_len_src, src_batch.shape[-1])\n",
    "    max_seq_len_tgt = max(max_seq_len_tgt, tgt_batch.shape[-1])\n",
    "\n",
    "max_seq_len_src, max_seq_len_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tranformer import Embedding, Transformer\n",
    "\n",
    "src_embedding = Embedding(d_model=512,\n",
    "                          vocab_size=len(src_vocab),\n",
    "                          max_seq_len=max_seq_len_src+10)\n",
    "tgt_embedding = Embedding(d_model=512,\n",
    "                          vocab_size=len(tgt_vocab),\n",
    "                          max_seq_len=max_seq_len_tgt+10)\n",
    "transformer = Transformer(output_size=len(tgt_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sathyakrishnansuresh/miniforge3/envs/tf/lib/python3.11/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/Users/sathyakrishnansuresh/miniforge3/envs/tf/lib/python3.11/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "Transformer                                             [32, 43, 10837]           --\n",
       "├─TransformerEncoder: 1-1                               [32, 46, 512]             --\n",
       "│    └─ModuleList: 2-1                                  --                        --\n",
       "│    │    └─TransformerEncoderLayer: 3-1                [32, 46, 512]             3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-2                [32, 46, 512]             3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-3                [32, 46, 512]             3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-4                [32, 46, 512]             3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-5                [32, 46, 512]             3,152,384\n",
       "│    │    └─TransformerEncoderLayer: 3-6                [32, 46, 512]             3,152,384\n",
       "├─TransformerDecoder: 1-2                               [32, 43, 512]             --\n",
       "│    └─ModuleList: 2-2                                  --                        --\n",
       "│    │    └─TransformerDecoderLayer: 3-7                [32, 43, 512]             4,204,032\n",
       "│    │    └─TransformerDecoderLayer: 3-8                [32, 43, 512]             4,204,032\n",
       "│    │    └─TransformerDecoderLayer: 3-9                [32, 43, 512]             4,204,032\n",
       "│    │    └─TransformerDecoderLayer: 3-10               [32, 43, 512]             4,204,032\n",
       "│    │    └─TransformerDecoderLayer: 3-11               [32, 43, 512]             4,204,032\n",
       "│    │    └─TransformerDecoderLayer: 3-12               [32, 43, 512]             4,204,032\n",
       "├─Linear: 1-3                                           [32, 43, 10837]           5,559,381\n",
       "=========================================================================================================\n",
       "Total params: 49,697,877\n",
       "Trainable params: 49,697,877\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 1.59\n",
       "=========================================================================================================\n",
       "Input size (MB): 5.83\n",
       "Forward/backward pass size (MB): 1063.01\n",
       "Params size (MB): 198.79\n",
       "Estimated Total Size (MB): 1267.64\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(transformer,\n",
    "        input_size=[(32, max_seq_len_src, 512), (32, max_seq_len_tgt, 512)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_src_mask(src):\n",
    "    src_mask = src!=PAD_IDX\n",
    "    return src_mask.unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "def create_trg_mask(trg):\n",
    "    size = trg.size(-1) - 1\n",
    "    # trg_padding_mask = (trg!=PAD_IDX).unsqueeze(1).unsqueeze(2)\n",
    "    trg_mask = torch.tril(torch.ones(size, size)).expand(trg.size(0), 1, size, size)\n",
    "    return trg_mask\n",
    "\n",
    "def create_mask(src, trg):\n",
    "    return create_src_mask(src), create_trg_mask(trg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: Transformer,\n",
    "          dataloader: torch.utils.data.DataLoader,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          optimizer: torch.optim.Optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    losses = 0\n",
    "\n",
    "    for batch, (src_batch, tgt_batch) in enumerate(tqdm(dataloader)):\n",
    "        \n",
    "        src_mask, tgt_mask = create_mask(src_batch, tgt_batch)\n",
    "        src_embedded = src_embedding(src_batch)\n",
    "        tgt_embedded = tgt_embedding(tgt_batch)\n",
    "        \n",
    "        logits = transformer(src_embedded, tgt_embedded[:, :-1], src_mask, tgt_mask)\n",
    "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_batch[:, 1:].reshape(-1))\n",
    "        losses += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch%5==0:\n",
    "            sample_translation = logits[0]\n",
    "            sample_translation = torch.argmax(sample_translation, dim=1).squeeze().numpy()\n",
    "            sample_translation = \" \".join(tgt_vocab.get_itos()[i] for i in list(sample_translation))\n",
    "            print(f\"Batch: {batch} sample translation: {sample_translation}\")\n",
    "\n",
    "    losses /= len(list(dataloader))\n",
    "    return losses        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef457d5c3b0f45198d87794e455c3085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sathyakrishnansuresh/miniforge3/envs/tf/lib/python3.11/site-packages/torch/utils/data/datapipes/iter/combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 sample translation: catamaran Warrior touch played Golden devil referees Olympics Pipe attentively pants contains slopes otuside otuside otuside haired operates otuside otuside barricade otuside ropes\n",
      "Batch: 5 sample translation: A a . a a a . a a a <eos> a a a a a a a a a a a a a a\n",
      "Batch: 10 sample translation: A . a a . a a . . <eos> a . a a . . . a a . .\n",
      "Batch: 15 sample translation: A a . . a . . a . . . . . . . . . . . . . . . .\n",
      "Batch: 20 sample translation: A man a a a a a a a a <eos> a a a a a . a a a a a\n",
      "Batch: 25 sample translation: A . . . . . . . . . . . . <eos> . . . . . . .\n",
      "Batch: 30 sample translation: A man . . a . . a . . . . . . . . . . . . . . .\n",
      "Batch: 35 sample translation: A . . a a red . . <eos> . . . . . . . . . . . . . . .\n",
      "Batch: 40 sample translation: A . . . . . . . <eos> . . . . . . . . . . . . . .\n",
      "Batch: 45 sample translation: A . in . . in in a . . a . . <eos> . . . . . . . . . . . .\n",
      "Batch: 50 sample translation: A . in . a a . <eos> . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 55 sample translation: A man in in a red . a . . . <eos> . . . . . . . . . . . . . .\n",
      "Batch: 60 sample translation: A man a is a a red . . <eos> . . . . . . . . . . . . . . .\n",
      "Batch: 65 sample translation: A man in a red . a red . . <eos> . . . . . . . . . . . . . . .\n",
      "Batch: 70 sample translation: A a in in a red . a . . . street . <eos> . . . . . . . . . . .\n",
      "Batch: 75 sample translation: A man in a red is a a a in red . . <eos> a . . . . .\n",
      "Batch: 80 sample translation: A in a . . . . . red . . <eos> . . . . . . . . . .\n",
      "Batch: 85 sample translation: A man in a woman in is standing . . . <eos> . . . . . . . . . . .\n",
      "Batch: 90 sample translation: A men boy in sitting on a blue . . . <eos> . . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 95 sample translation: A in a a is on a a a . <eos> a . . . . . . . . . .\n",
      "Batch: 100 sample translation: A woman in in a street . a street . . . <eos> . . . . . . . . .\n",
      "Batch: 105 sample translation: A woman boy in a of . a red . <eos> . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 110 sample translation: A man girl in a red . a red . <eos> . . . . . . . . . . . . . . .\n",
      "Batch: 115 sample translation: A in a blue . . a . . . a red . <eos> . . . . . . . . .\n",
      "Batch: 120 sample translation: Two men in a in in . <eos> in . . . . . . in in . . . in in . . . . . in\n",
      "Batch: 125 sample translation: A man boy is a blue . a a . <eos> . . . . . . . . .\n",
      "Batch: 130 sample translation: Two men woman dog are with on . . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 135 sample translation: A young in a red . in the . . <eos> . . . . . . . . . .\n",
      "Batch: 140 sample translation: Two in in white . a . <eos> . . . . . . . . . . . . . . . . .\n",
      "Batch: 145 sample translation: A man in a white in in a in a <eos> . . . . . . . . . . . .\n",
      "Batch: 150 sample translation: A woman in a woman is a a and . . a blue . . . . . . . . .\n",
      "Batch: 155 sample translation: A young in in black of <eos> . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 160 sample translation: A woman on a is on a and . and . . . . . . . . . . . .\n",
      "Batch: 165 sample translation: A man wearing a and sitting on the water . a . on . shirt . <eos> . . . . . . . .\n",
      "Batch: 170 sample translation: A woman wearing with with a red . . in a red shirt . <eos> . . . . . . . . . . . . . . .\n",
      "Batch: 175 sample translation: A group of people in a red in is in in in in a in dog a the . in <eos> in . . .\n",
      "Batch: 180 sample translation: A young is in the water . . . . . . . . . . . . . . .\n",
      "Batch: 185 sample translation: Two woman the and with a . . the grass . . . . <eos> . . . . . . . . . .\n",
      "Batch: 190 sample translation: Three in sitting a are a . <eos> in . . . . . . . . . . . .\n",
      "Batch: 195 sample translation: A man in a man a is a . a blue . . shirt . shirt . <eos> . .\n",
      "Batch: 200 sample translation: A young boy in in water with a red . <eos> in . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 205 sample translation: Two in a and and a a a . a . <eos> a . . a . . .\n",
      "Batch: 210 sample translation: Two men and standing on white standing and and <eos> and and and and and and and and and and\n",
      "Batch: 215 sample translation: Two men boy in the . in <eos> . . . . . . . . . . . . . .\n",
      "Batch: 220 sample translation: A dog dog is a white . <eos> . . . . . . . . . . .\n",
      "Batch: 225 sample translation: A young on is on the street . . <eos> on . . . . . . . . . . . . . . .\n",
      "Batch: 230 sample translation: Three men in in a orange in in . <eos> in in in in in in in in in in\n",
      "Batch: 235 sample translation: A young with with a a a blue . . <eos> . . . . . . . . . . . . . .\n",
      "Batch: 240 sample translation: A woman is a woman in on a . . a street of a blue . . <eos> . . . . . . . . . . . . . . . . . .\n",
      "Batch: 245 sample translation: A woman in a woman is is in to a blue . <eos> . . . . . . .\n",
      "Batch: 250 sample translation: A woman is holding a a woman in on on <eos> on on on on on a on on on on a on on on on\n",
      "Batch: 255 sample translation: A woman in a orange shirt is holding on the . . <eos> . . . . . . . . . .\n",
      "Batch: 260 sample translation: A young in a blue shirt is in a water . <eos> . . . . . . . .\n",
      "Batch: 265 sample translation: A woman in a woman shirt is in in in a blue . . <eos> a . . . . . .\n",
      "Batch: 270 sample translation: A man is on a blue . the street . . . . . . . . . . . . . .\n",
      "Batch: 275 sample translation: A dog is on a street of a outdoor . . . <eos> . . . . . .\n",
      "Batch: 280 sample translation: A man is a blue shirt is . blue . in . <eos> . . . . . . . .\n",
      "Batch: 285 sample translation: A group is on A on the street . <eos> . . . . . . . . . . . . . . . .\n",
      "Batch: 290 sample translation: A man girl is a one woman is on a . <eos> a . . a . . . . . . . . . . . . . . . . .\n",
      "Batch: 295 sample translation: A man in in a a . . the in . <eos> . . . . . . . . .\n",
      "Batch: 300 sample translation: A man man is in <eos> the at at . . . . . . . a . . . . . . . . . . .\n",
      "Batch: 305 sample translation: A dog is in a black in people . a . . <eos> . . . . . . . . .\n",
      "Batch: 310 sample translation: Three and a a a large on . . . . . . . . . . . . . . .\n",
      "Batch: 315 sample translation: A dog is . the grass . . . <eos> . . . . . . . . . . . . . . .\n",
      "Batch: 320 sample translation: The are sitting on a red grass with of a and one grass and <eos> with , . , . . . . . . . , . ,\n",
      "Batch: 325 sample translation: A man in standing in a with with a with with a . <eos> with . . . . . . . .\n",
      "Batch: 330 sample translation: A with a with standing a shirt on and <eos> and and and and and and and and and and and and\n",
      "Batch: 335 sample translation: A group a of red is of a on a picture . <eos> . . . . . . . . . . .\n",
      "Batch: 340 sample translation: A man is holding at a man child . <eos> a . . . . . . . . .\n",
      "Batch: 345 sample translation: A young girl in in the snow . . . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 350 sample translation: A group of people are a a red . . <eos> . . . . . . . . . . . .\n",
      "Batch: 355 sample translation: Three people . are . through . <eos> . . . . . . . . . . . . . . . . .\n",
      "Batch: 360 sample translation: A in a shirt is in a . . . . . . . . . . . . .\n",
      "Batch: 365 sample translation: A man in a is playing in the man . <eos> . . . . . . . . . .\n",
      "Batch: 370 sample translation: A little in in in a large . the the large . . <eos> . . . . . . . . . .\n",
      "Batch: 375 sample translation: A little in a blue shirt white shirt . running . <eos> a . . . . . . . . . . . .\n",
      "Batch: 380 sample translation: The dog dog white dog is in mouth in the snow . <eos> . . . . . . .\n",
      "Batch: 385 sample translation: A group of people and haired in are and a large . <eos> . . . . . . . .\n",
      "Batch: 390 sample translation: A little player is down the snow . <eos> . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 395 sample translation: Two men and a with on a large with . . <eos> . . . . . . . . . .\n",
      "Batch: 400 sample translation: A man is off a snow . a large building . . <eos> . . . . . . . . . . . . .\n",
      "Batch: 405 sample translation: A girl is a is a a red . <eos> . . . . . . . . . . . .\n",
      "Batch: 410 sample translation: A man is a man street . <eos> a . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 415 sample translation: A woman is is another in the . <eos> . . . . . . . . . . . .\n",
      "Batch: 420 sample translation: A with the the . he people with . <eos> . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 425 sample translation: A group of people are are on a large . . . . . . . . . . . . . . . . .\n",
      "Batch: 430 sample translation: A little and a red . to a . of a . . <eos> . . . . . .\n",
      "Batch: 435 sample translation: A woman woman is a shirt is a orange shirt . <eos> a . . . . . . . . . . . . .\n",
      "Batch: 440 sample translation: Two people are in in the air . <eos> . . . . . . . . . . . . . .\n",
      "Batch: 445 sample translation: A older man and on the air and and air of <eos> and and and and and and and and . and . and . and and and and and and and and and and and and and and and\n",
      "Batch: 450 sample translation: A girl is a on on one . on the air . . <eos> . . . . .\n",
      "Batch: 455 sample translation: A girl is a white with a white . a . <eos> . . . . . . . . .\n",
      "Batch: 460 sample translation: The person women is on the street . a large . . <eos> . . . . . . . . . . .\n",
      "Batch: 465 sample translation: A woman in a black shirt is on a . <eos> . . . . . . . . . . . . . . . .\n",
      "Batch: 470 sample translation: A young on and a dog on walking <eos> . a a large . <eos> . . .\n",
      "Batch: 475 sample translation: A little - haired girl in in the bike and and . <eos> and and and and and and and and and and and and and and and and\n",
      "Batch: 480 sample translation: Two men are on a sidewalk . . <eos> . . . . . . . . . . . . . . . .\n",
      "Batch: 485 sample translation: A men are are on on . <eos> on on at on . on on on on on on on . . on on\n",
      "Batch: 490 sample translation: Two men women are . . the . the . a <eos> . . . . . . . . . .\n",
      "Batch: 495 sample translation: A man and a woman is to . <eos> . . . . . . . . . . . . . .\n",
      "Batch: 500 sample translation: A and and a large and a large . <eos> . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 505 sample translation: The little is playing of people street . . the <eos> . . . . . . . . . . .\n",
      "Batch: 510 sample translation: People in a on playing on a outdoor . <eos> . . . . . . . . . . . . . . . . .\n",
      "Batch: 515 sample translation: Two people are black jacket a playing a a table . <eos> . . . . . . . . . . . .\n",
      "Batch: 520 sample translation: Two people - and on a street and and and <eos> and and and and and and and and and and and and and and and\n",
      "Batch: 525 sample translation: A on on a street . <eos> . . . . . . . . . . . .\n",
      "Batch: 530 sample translation: People are on a on on on a red . <eos> . . . . . . . . . . . .\n",
      "Batch: 535 sample translation: A couple in a red shirt is . <eos> . . . . . . . . . . . . . . . .\n",
      "Batch: 540 sample translation: A child is a child child with a child . <eos> . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 545 sample translation: A man and a with the on the large . . <eos> . . . . . . . . . . .\n",
      "Batch: 550 sample translation: Two men and one is , a is , one standing in a in <eos> in , , , , in , , ,\n",
      "Batch: 555 sample translation: A in on in a in a red . . <eos> . . . . . . . . . . . . . . .\n",
      "Batch: 560 sample translation: A man with a white shirt with a white . <eos> with . . . . . . . . . . . . . . . . .\n",
      "Batch: 565 sample translation: A girl in in and holding in the snow . <eos> . . . . . . . . . . . . . .\n",
      "Batch: 570 sample translation: A man in a a street . in a . <eos> . . . . . . . . . .\n",
      "Batch: 575 sample translation: Two men are walking the . the street . <eos> . . . . . . . . . . . . . . . . . .\n",
      "Batch: 580 sample translation: A couple is down a street street . <eos> a . . . . . . . . . . .\n",
      "Batch: 585 sample translation: A man in a blue shirt is a shirt . blue . <eos> a . . . . . . . . .\n",
      "Batch: 590 sample translation: A woman and a and standing a man . <eos> . . . . . . . . . . . . .\n",
      "Batch: 595 sample translation: Two women are at are on a street street . . . <eos> . . . . . . .\n",
      "Batch: 600 sample translation: A with on a of street . a city . <eos> . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 605 sample translation: A man man in a white shirt is a is on a white . <eos> . . . . . . . . . . . . .\n",
      "Batch: 610 sample translation: A girl is is a red of a . . <eos> . . . . . . . . . . . . . .\n",
      "Batch: 615 sample translation: A man and woman with a the street . . <eos> . . . . . . . . . . . . . .\n",
      "Batch: 620 sample translation: A people are down to a the . one a a old . <eos> . . . . . . . . .\n",
      "Batch: 625 sample translation: Two girls girls are on a girl in front of a sort . <eos> . . . . . . . . . .\n",
      "Batch: 630 sample translation: A group on people women are a orange . . <eos> . . . . . . . . . . . . . . . . . .\n",
      "Batch: 635 sample translation: A men are in a shirt are are on a . . <eos> . . . . . . . .\n",
      "Batch: 640 sample translation: A are a street . a hands . a . <eos> . . . . . . . . . . .\n",
      "Batch: 645 sample translation: A man is sitting at a man . <eos> and . . and . and . . . . and and . . .\n",
      "Batch: 650 sample translation: People are in of large . <eos> in in in in in in in in in in in in in in . in\n",
      "Batch: 655 sample translation: A young man with a large with in a . <eos> . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 660 sample translation: A with a and a . . a . a . . . . . . . . . . . . . . .\n",
      "Batch: 665 sample translation: A older man and sitting on the sidewalk . the cellphone . . <eos> . . . . . . . . .\n",
      "Batch: 670 sample translation: A with down a - is . the water . . <eos> . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 675 sample translation: A man and a woman are outside a other people a . <eos> . . . . . . . . . .\n",
      "Batch: 680 sample translation: A woman in a black top is on a man in a man . . <eos> . . . . . .\n",
      "Batch: 685 sample translation: Three people sitting on a street . a street . <eos> . . . . . . . . . . . . . .\n",
      "Batch: 690 sample translation: A young is a to a outdoor . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 695 sample translation: A woman in on a sidewalk in in a sidewalk . <eos> . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 700 sample translation: A man in playing a man of a camera . hand . <eos> . . . . . . . . .\n",
      "Batch: 705 sample translation: Three young girl are on a sidewalk . <eos> . . . . . . . . . . . . . .\n",
      "Batch: 710 sample translation: A walking on a ball while a . a women . . <eos> . . . . . . . . .\n",
      "Batch: 715 sample translation: A couple in taking a large in of people . a hand . <eos> . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 720 sample translation: Two women in with in a and a woman a shirt a . . <eos> and . . , . , ,\n",
      "Batch: 725 sample translation: Two people girls are walking a large girl . <eos> . . . . . . . . . . . . . . . .\n",
      "Batch: 730 sample translation: A couple is in the is at . <eos> to . . . . . . . . . . . . . .\n",
      "Batch: 735 sample translation: A woman in a blue shirt top is . . <eos> . . . . . . . .\n",
      "Batch: 740 sample translation: A young boy in his a blue . <eos> . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 745 sample translation: Four people are in the . . . <eos> . . and . . . . . . . . . . . . . . . . .\n",
      "Batch: 750 sample translation: A Asian couple is his . he is . . bike . . <eos> . . . . . . . . . .\n",
      "Batch: 755 sample translation: A man in in in playing a camera . <eos> . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 760 sample translation: A group of people in in in the water . <eos> . . . . . . . . . . . . . .\n",
      "Batch: 765 sample translation: Two men in in woman , a man a a a water men <eos> in . . in . in . . . . . .\n",
      "Batch: 770 sample translation: A man is a man child on to a sidewalk . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 775 sample translation: Two men and are playing a and and one and playing their . . <eos> . . . .\n",
      "Batch: 780 sample translation: A older of is down a jumping a a field . <eos> . . . . . . . . .\n",
      "Batch: 785 sample translation: A man is man man is man is <eos> a a a a . . a a . . . . a\n",
      "Batch: 790 sample translation: A man is on a bench and a . sidewalk . a sidewalk . <eos> . . . . . . . . . . .\n",
      "Batch: 795 sample translation: A man - haired man in a blue shirt to in head . <eos> . . . . . . . . . . . .\n",
      "Batch: 800 sample translation: A man in a orange shirt is on the street of a street . <eos> . . . . . . . . . . . . . . .\n",
      "Batch: 805 sample translation: A male in in a yellow and on is on the on the ground . <eos> . . . . . . . . . . . .\n",
      "Batch: 810 sample translation: A person in a in in in a of a large . . a . <eos> . . . . . . .\n",
      "Batch: 815 sample translation: A baseball is with a and with and white and . . <eos> . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 820 sample translation: Three men are on a field in the field . <eos> . . . . . . . . . . . .\n",
      "Batch: 825 sample translation: Two people arts with with a soccer , a red and at . <eos> . . . . . . .\n",
      "Batch: 830 sample translation: Four football are playing the hands . . . <eos> . . . . . . . . . .\n",
      "Batch: 835 sample translation: A on on bike a bike of . . . . . . . . . . . . . . . . . .\n",
      "Batch: 840 sample translation: A man is a bike on a on a bike wall . <eos> . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 845 sample translation: A little girl is the to of through to <eos> the . . . . . . . . . . .\n",
      "Batch: 850 sample translation: A man in a and - shirt is a white . . <eos> . . . . . . . . . . . . . . . . . .\n",
      "Batch: 855 sample translation: The players is in the the ball of the . . <eos> . . . . . . . . . .\n",
      "Batch: 860 sample translation: A man is down a bike a field . . <eos> . . . . . . . . . . .\n",
      "Batch: 865 sample translation: A girl girl is a girl in in the background . <eos> . . . . . . . . . .\n",
      "Batch: 870 sample translation: A woman is a black and and a sitting a table . a microphone . . <eos> . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 875 sample translation: Two women are soccer large . a . <eos> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n",
      "Batch: 880 sample translation: A older is ready to a a large in in front large . . <eos> a . . . . . . . . . .\n",
      "Batch: 885 sample translation: A young is a game on a field field . <eos> . . . . . . . . . .\n",
      "Batch: 890 sample translation: Two man are are one man and a woman are are a the . <eos> . . . . . . . . .\n",
      "Batch: 895 sample translation: A baseball is is riding on a . <eos> . . . . . . . . . . . . . . . . . .\n",
      "Batch: 900 sample translation: A girl girl is a girl while a head . . on . <eos> . . . . . . . . . . .\n",
      "Batch: 905 sample translation: A group of people are on a large . <eos> . . . . . . . . . . . . .\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "losses = train(transformer, train_dataloader, loss_fn, optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
