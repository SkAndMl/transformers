{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/IMDB.csv\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing NSP dataset and Masked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_PERCENTANGE = 0.15\n",
    "MASK_PROBABILITY = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked\n",
      "I thought the movie plot was very good and [MASK] everyone else enjoys it to! Be sure and rent it!! Also they had some great soccer scenes for all those soccer players! :)\n",
      "NSP: 0\n",
      "I thought the movie plot was very good and hope everyone else enjoys it to! Be sure and rent it!! Also they had some great soccer scenes for all those soccer players! :). [SEP] I thought the movie plot was very good and hope everyone else enjoys it to! Be sure and rent it!! Also they had some great soccer scenes for all those soccer players! :)\n",
      "67\n"
     ]
    }
   ],
   "source": [
    "sents = df.iloc[random.randint(0, df.shape[0]), 0].split(\". \")\n",
    "idx = random.randint(0, len(sents)-1)\n",
    "sent1 = sents[idx]\n",
    "\n",
    "sent1 = sent1.split(\" \")\n",
    "mask_idx = random.randint(0, len(sent1)-1)\n",
    "if random.random() < MASK_PROBABILITY:\n",
    "    sent1[mask_idx] = \"[MASK]\"\n",
    "    print(\"Masked\")\n",
    "    print(\" \".join(sent1))\n",
    "else:\n",
    "    rand_token = random.randint(0, len(sent1)-1)\n",
    "    sent1[mask_idx] = sent1[rand_token]\n",
    "    print(\"Replaced with random token\")\n",
    "    print(\" \".join(sent1))\n",
    "\n",
    "if random.random() <= 0.5:\n",
    "    nsp_sents = \". [SEP] \".join(sents[idx: idx+2])\n",
    "    print(\"NSP: 1\")\n",
    "    print(nsp_sents)\n",
    "    print(len(nsp_sents.split(\" \")))\n",
    "else:\n",
    "    nsp_sents = sents[idx] + \". [SEP] \" + sents[random.randint(idx+1, len(sents))-1]\n",
    "    print(\"NSP: 0\")\n",
    "    print(nsp_sents)\n",
    "    print(len(nsp_sents.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLS_TOKEN = 0\n",
    "SEP_TOKEN = 1\n",
    "MASK_TOKEN = 2\n",
    "PAD_TOKEN = 3 \n",
    "UNK_TOKEN = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "\n",
    "def build_vocab(data_iter):\n",
    "    for sentence in data_iter:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "vocab = build_vocab_from_iterator(build_vocab(df[\"review\"].to_list()),\n",
    "                                  min_freq=2,\n",
    "                                  specials=[\"[CLS]\", \"[SEP]\", \"[MASK]\", \"[PAD]\", \"<UNK>\"],\n",
    "                                  special_first=True)\n",
    "vocab.set_default_index(UNK_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89854"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5002261, 4997739)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_ = 0\n",
    "more_ = 0\n",
    "for _ in range(10000000):\n",
    "    if random.random()<=0.5:\n",
    "        less_ += 1\n",
    "    else:\n",
    "        more_ += 1\n",
    "less_, more_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`random.random()` generates an equal distribution and it can be used for the NSP task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data_setup.py\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class IMDBBERTDataset(Dataset):\n",
    "\n",
    "    NSP_PERCENTAGE = 0.50\n",
    "\n",
    "    CLS_TOKEN = 0\n",
    "    SEP_TOKEN = 1\n",
    "    MASK_TOKEN = 2\n",
    "    PAD_TOKEN = 3 \n",
    "    UNK_TOKEN = 4\n",
    "\n",
    "    def __init__(self,\n",
    "                 path: str,\n",
    "                 max_sent_len: int=50) -> None:\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(path)\n",
    "        self.tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "        self.vocab = build_vocab_from_iterator(self._build_vocab(self.df[\"review\"].to_list()),\n",
    "                                               min_freq=2,\n",
    "                                               specials=[\"[CLS]\", \"[SEP]\", \"[MASK]\", \"[PAD]\", \"<UNK>\"],\n",
    "                                               special_first=True)\n",
    "        self.vocab.set_default_index(self.UNK_TOKEN)\n",
    "        self.max_sent_len = max_sent_len\n",
    "        self.token_ids = []\n",
    "        self.masked_token = []\n",
    "        self.masked_idx = []\n",
    "        self.is_next = []\n",
    "        self.segment_tokens = []\n",
    "        self._prepare_data()\n",
    "        \n",
    "\n",
    "    def _prepare_data(self) -> None:\n",
    "        for i in range(self.df.shape[0]):\n",
    "            try:\n",
    "                sentences = self.df.iloc[i, 0].split(\". \")\n",
    "                if random.random() <= self.NSP_PERCENTAGE:\n",
    "                    rand_idx = random.randint(0, len(sentences)-2)\n",
    "                    sentences = sentences[rand_idx:rand_idx+2]\n",
    "                    is_next = 0\n",
    "                else:\n",
    "                    rand_idx = random.randint(1, len(sentences)-1)\n",
    "                    sentences = [sentences[rand_idx], sentences[rand_idx-1]]\n",
    "                    is_next = 1\n",
    "                \n",
    "                sentences = [\"[CLS]\"] + self.tokenizer(sentences[0]) + [\"[SEP]\"] + self.tokenizer(sentences[1])\n",
    "                if len(sentences) < self.max_sent_len:\n",
    "                    while len(sentences) < self.max_sent_len:\n",
    "                        sentences += [\"[PAD]\"]\n",
    "                else:\n",
    "                    sentences = sentences[:self.max_sent_len]\n",
    "                \n",
    "                sep_idx = sentences.index(\"[SEP]\")\n",
    "                segment_token = [0]*(sep_idx+1) + [1]*(len(sentences)-1-sep_idx)\n",
    "\n",
    "                # assert len(segment_token) == len(sentences), f\"Length not equal, sep_idx: {sep_idx} \"\n",
    "\n",
    "                token_ids = self.vocab(sentences)\n",
    "                mask_token, mask_idx = self.SEP_TOKEN, -1\n",
    "                while mask_token == self.SEP_TOKEN:\n",
    "                    mask_idx = random.randint(1, len(token_ids)-1)\n",
    "                    mask_token = token_ids[mask_idx]\n",
    "                token_ids[mask_idx] = self.MASK_TOKEN\n",
    "                self.token_ids.append(token_ids)\n",
    "                self.masked_token.append(mask_token)\n",
    "                self.masked_idx.append(mask_idx)\n",
    "                self.segment_tokens.append(segment_token)\n",
    "                self.is_next.append(is_next)\n",
    "            except:\n",
    "                pass\n",
    " \n",
    "        self.bert_df = pd.DataFrame(data={\n",
    "            \"token_ids\" : self.token_ids,\n",
    "            \"segment_tokens\" : self.segment_tokens,\n",
    "            \"masked_token\" : self.masked_token,\n",
    "            \"masked_idx\" : self.masked_idx,\n",
    "            \"is_next\" : self.is_next\n",
    "        })\n",
    "        \n",
    "    def _build_vocab(self, data_iter):\n",
    "        for sentence in data_iter:\n",
    "            yield self.tokenizer(sentence)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, int, int, int]:\n",
    "        token_ids = self.token_ids[index]\n",
    "        segment_tokens = self.segment_tokens[index]\n",
    "        masked_token = self.masked_token[index]\n",
    "        masked_idx = self.masked_idx[index]\n",
    "        is_next = self.is_next[index]\n",
    "        return torch.tensor(token_ids), torch.tensor(segment_tokens), masked_token, masked_idx, is_next\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.bert_df.shape[0]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ds = IMDBBERTDataset(path=\"../data/IMDB.csv\")\n",
    "    print(f\"Shape: {ds.bert_df.shape}\")\n",
    "    print(ds.bert_df.head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT EMBEDDING"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert embedding -> Segment Embedding + Positional Embedding + Token Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing bert_embedding.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bert_embedding.py\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "class BERTEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int=512,\n",
    "                 vocab_size: int=1000,\n",
    "                 max_seq_len: int=100,\n",
    "                 dropout: float=0.1) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                            embedding_dim=d_model)\n",
    "        self.pe = torch.zeros(size=(max_seq_len, d_model),\n",
    "                              requires_grad=False)\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for dim in range(d_model):\n",
    "                if pos%2==0:\n",
    "                    self.pe[pos, dim] = math.sin(pos//(10000**(2*dim//d_model)))\n",
    "                else:\n",
    "                    self.pe[pos, dim] = math.cos(pos//(10000**(2*dim//d_model)))\n",
    "\n",
    "        self.segment_embedding = nn.Embedding(num_embeddings=2,\n",
    "                                              embedding_dim=d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"BERTEmbedding(d_model={self.d_model}, vocab_size={self.vocab_size})\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"BERTEmbedding(d_model={self.d_model}, vocab_size={self.vocab_size})\"\n",
    "\n",
    "    def forward(self, \n",
    "                x: torch.Tensor,\n",
    "                segment_tokens: torch.Tensor) -> torch.Tensor:\n",
    "        # x -> [batch_size, max_seq_len]\n",
    "        token_embeddings = self.token_embedding(x)\n",
    "        position_encoding = self.pe[:x.shape[1], :].unsqueeze(0) # positional_encoding -> [1, max_seq_len, d_model]\n",
    "        segment_embedding = self.segment_embedding(segment_tokens)\n",
    "        return self.dropout(token_embeddings + position_encoding + segment_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_ids</th>\n",
       "      <th>segment_tokens</th>\n",
       "      <th>masked_token</th>\n",
       "      <th>masked_idx</th>\n",
       "      <th>is_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 6805, 87, 6, 19, 12, 36, 8, 140, 26, 5, 89...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>13060</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 111, 113, 21, 121, 1964, 76, 294, 396, 25,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>91</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 28, 133, 12, 2, 6, 29, 5, 448, 12, 2044, 9...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>4391</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 272, 26, 5, 678, 25, 3627, 97, 58, 3081, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>38</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 151, 251, 872, 12, 36, 659, 5, 147, 305, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>363</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41841</th>\n",
       "      <td>[0, 327, 49, 1998, 67, 9, 207, 13, 544, 6, 247...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41842</th>\n",
       "      <td>[0, 130, 38, 4299, 12, 51, 15839, 9, 41, 12, 5...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41843</th>\n",
       "      <td>[0, 2, 89, 31, 539, 74, 4, 22, 13, 19, 52, 36,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41844</th>\n",
       "      <td>[0, 486, 31, 966, 26, 15, 11, 39, 8, 399, 6, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>24</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41845</th>\n",
       "      <td>[0, 32, 3200, 225, 87, 8, 35585, 74, 34, 72, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>88</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41846 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               token_ids  \\\n",
       "0      [0, 6805, 87, 6, 19, 12, 36, 8, 140, 26, 5, 89...   \n",
       "1      [0, 111, 113, 21, 121, 1964, 76, 294, 396, 25,...   \n",
       "2      [0, 28, 133, 12, 2, 6, 29, 5, 448, 12, 2044, 9...   \n",
       "3      [0, 272, 26, 5, 678, 25, 3627, 97, 58, 3081, 1...   \n",
       "4      [0, 151, 251, 872, 12, 36, 659, 5, 147, 305, 1...   \n",
       "...                                                  ...   \n",
       "41841  [0, 327, 49, 1998, 67, 9, 207, 13, 544, 6, 247...   \n",
       "41842  [0, 130, 38, 4299, 12, 51, 15839, 9, 41, 12, 5...   \n",
       "41843  [0, 2, 89, 31, 539, 74, 4, 22, 13, 19, 52, 36,...   \n",
       "41844  [0, 486, 31, 966, 26, 15, 11, 39, 8, 399, 6, 1...   \n",
       "41845  [0, 32, 3200, 225, 87, 8, 35585, 74, 34, 72, 1...   \n",
       "\n",
       "                                          segment_tokens  masked_token  \\\n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         13060   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            91   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          4391   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...            38   \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...           363   \n",
       "...                                                  ...           ...   \n",
       "41841  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            24   \n",
       "41842  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            18   \n",
       "41843  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ...            59   \n",
       "41844  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            24   \n",
       "41845  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            88   \n",
       "\n",
       "       masked_idx  is_next  \n",
       "0              14        0  \n",
       "1              22        0  \n",
       "2               4        1  \n",
       "3              19        1  \n",
       "4              39        1  \n",
       "...           ...      ...  \n",
       "41841          27        1  \n",
       "41842          17        1  \n",
       "41843           1        1  \n",
       "41844          37        1  \n",
       "41845          42        0  \n",
       "\n",
       "[41846 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data_setup import IMDBBERTDataset\n",
    "ds = IMDBBERTDataset(\"../data/IMDB.csv\")\n",
    "ds.bert_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89854"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(ds.vocab)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids shape: torch.Size([32, 50])\n",
      "embedding shape: torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from bert_embedding import BERTEmbedding\n",
    "data_loader = DataLoader(dataset=ds,\n",
    "                         batch_size=32,\n",
    "                         shuffle=True)\n",
    "batch = next(iter(data_loader))\n",
    "bert_embedding = BERTEmbedding(d_model=512,\n",
    "                               vocab_size=VOCAB_SIZE)\n",
    "with torch.inference_mode():\n",
    "    token_ids, segment_tokens = batch[0], batch[1]\n",
    "    embedding = bert_embedding(token_ids, segment_tokens)\n",
    "    print(f\"token_ids shape: {token_ids.shape}\")\n",
    "    print(f\"embedding shape: {embedding.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BERT model is made up of just the encoder part of the transformer architecture.\n",
    "* I am going to build the BERT-Base model in this kernel\n",
    "* Parameters for BERT-Base: \n",
    "    * Number of encoder layers   **L : 12**\n",
    "    * Model Dimension            **H : 768**\n",
    "    * Number of Attention heads  **A : 12** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ../scripts/scripts.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ../scripts/scripts.py\n",
    "\n",
    "def create_padding_mask(batch: torch.Tensor,\n",
    "                        padding_idx: int=0) -> torch.Tensor:\n",
    "    # batch -> [batch_size, max_seq_len]\n",
    "    mask = batch != padding_idx\n",
    "    return mask.unsqueeze(1).unsqueeze(2) # mask -> [batch_size, 1, 1, max_seq_len] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 768])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.rand(32, 50, 768)\n",
    "masked_idx = torch.tensor([random.randint(0, 49) for _ in range(32)])\n",
    "masked_tokens = batch[range(len(masked_idx)), masked_idx]\n",
    "masked_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.scripts import TransformerEncoderLayer, TransformerEncoder, create_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "import torch\n",
    "from torch import nn\n",
    "from typing import Tuple\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from scripts.scripts import TransformerEncoder\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers: int=12,\n",
    "                 d_model: int=768,\n",
    "                 num_heads: int=12,\n",
    "                 vocab_size: int=1000,\n",
    "                 d_ff: int=2048,\n",
    "                 attn_dropout: float=0.1,\n",
    "                 ff_dropout: float=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.bert_encoder = TransformerEncoder(num_encoders=num_layers,\n",
    "                                               d_model=d_model,\n",
    "                                               num_heads=num_heads,\n",
    "                                               d_ff=d_ff,\n",
    "                                               attn_dropout=attn_dropout,\n",
    "                                               ff_dropout=ff_dropout)\n",
    "        self.masked_block = nn.Linear(in_features=d_model,\n",
    "                                      out_features=vocab_size)\n",
    "        self.nsp_block = nn.Linear(in_features=d_model,\n",
    "                                        out_features=2)\n",
    "    \n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"BERT(num_layers={self.num_layers}, d_model={self.d_model}, num_heads={self.num_heads})\"\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"BERT(num_layers={self.num_layers}, d_model={self.d_model}, num_heads={self.num_heads})\"\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                mask: torch.Tensor,\n",
    "                masked_idx: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        \"\"\"\n",
    "            x -> Input embedding, shape: [batch_size, max_seq_len, d_model]\n",
    "            mask -> Mask for padding, shape: [batch_size, 1, 1, max_seq_len]\n",
    "            masked_idx -> For each sequence a different index position has been masked\n",
    "            and the encoder's contextual representation of the masked token will be\n",
    "            used to predict the true token. To extract the representation for each \n",
    "            sequence the different index positions are passed in masked_idx which will \n",
    "            be used for indexing the representation. Shape: [batch_size]\n",
    "        \"\"\"\n",
    "        x = self.bert_encoder(x, mask) # [batch_size, max_seq_len, d_model]\n",
    "        masked_tokens = x[range(len(masked_idx)), masked_idx]\n",
    "\n",
    "        # For NSP prediction the BERT paper uses the '[CLS]' token which is the \n",
    "        # 0th index in each sequence and it is accessed by indexing '0' along the\n",
    "        # first dimension\n",
    "        nsp_logits = self.nsp_block(x[:, 0, :]) # x[:, 0, :] -> [batch_size, d_model]\n",
    "        masked_tokens_logits = self.masked_block(masked_tokens)\n",
    "        return masked_tokens_logits, nsp_logits\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
