{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/IMDB.csv\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_PERCENTANGE = 0.15\n",
    "MASK_PROBABILITY = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replaced with random token\n",
      "The vignettes are good (especially Speed Demon), but the best part of this movie is the super Demon), segment, in which Michael Jackson turns into a car, a robot, and finally a spaceship (and it's just as weird as it sounds)\n",
      "NSP: 1\n",
      "The vignettes are good (especially Speed Demon), but the best part of this movie is the super hero segment, in which Michael Jackson turns into a car, a robot, and finally a spaceship (and it's just as weird as it sounds). [SEP] Joe Pesci is hilarious, and has enough cool imagery and great music to entertain throughout!<br /><br />The real gem however is the incredible \"Smooth Criminal\" video, which makes the movie worth owning for that part alone!\n",
      "78\n"
     ]
    }
   ],
   "source": [
    "sents = df.iloc[random.randint(0, df.shape[0]), 0].split(\". \")\n",
    "idx = random.randint(0, len(sents)-1)\n",
    "sent1 = sents[idx]\n",
    "\n",
    "sent1 = sent1.split(\" \")\n",
    "mask_idx = random.randint(0, len(sent1)-1)\n",
    "if random.random() < MASK_PROBABILITY:\n",
    "    sent1[mask_idx] = \"[MASK]\"\n",
    "    print(\"Masked\")\n",
    "    print(\" \".join(sent1))\n",
    "else:\n",
    "    rand_token = random.randint(0, len(sent1)-1)\n",
    "    sent1[mask_idx] = sent1[rand_token]\n",
    "    print(\"Replaced with random token\")\n",
    "    print(\" \".join(sent1))\n",
    "\n",
    "if random.random() <= 0.5:\n",
    "    nsp_sents = \". [SEP] \".join(sents[idx: idx+2])\n",
    "    print(\"NSP: 1\")\n",
    "    print(nsp_sents)\n",
    "    print(len(nsp_sents.split(\" \")))\n",
    "else:\n",
    "    nsp_sents = sents[idx] + \". [SEP] \" + sents[random.randint(idx+1, len(sents))-1]\n",
    "    print(\"NSP: 0\")\n",
    "    print(nsp_sents)\n",
    "    print(len(nsp_sents.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLS_TOKEN = 0\n",
    "SEP_TOKEN = 1\n",
    "MASK_TOKEN = 2\n",
    "PAD_TOKEN = 3 \n",
    "UNK_TOKEN = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "\n",
    "def build_vocab(data_iter):\n",
    "    for sentence in data_iter:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "vocab = build_vocab_from_iterator(build_vocab(df[\"review\"].to_list()),\n",
    "                                  min_freq=2,\n",
    "                                  specials=[\"[CLS]\", \"[SEP]\", \"[MASK]\", \"[PAD]\", \"<UNK>\"],\n",
    "                                  special_first=True)\n",
    "vocab.set_default_index(UNK_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89854"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4998670, 5001330)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_ = 0\n",
    "more_ = 0\n",
    "for _ in range(10000000):\n",
    "    if random.random()<=0.5:\n",
    "        less_ += 1\n",
    "    else:\n",
    "        more_ += 1\n",
    "less_, more_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`random.random()` generates an equal distribution and it can be used for the NSP task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBBERTDataset(Dataset):\n",
    "\n",
    "    NSP_PERCENTAGE = 0.50\n",
    "\n",
    "    CLS_TOKEN = 0\n",
    "    SEP_TOKEN = 1\n",
    "    MASK_TOKEN = 2\n",
    "    PAD_TOKEN = 3 \n",
    "    UNK_TOKEN = 4\n",
    "\n",
    "    def __init__(self,\n",
    "                 path: str,\n",
    "                 max_sent_len: int=50) -> None:\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(path)\n",
    "        self.tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "        self.vocab = build_vocab_from_iterator(self._build_vocab(self.df[\"review\"].to_list()),\n",
    "                                               min_freq=2,\n",
    "                                               specials=[\"[CLS]\", \"[SEP]\", \"[MASK]\", \"[PAD]\", \"<UNK>\"],\n",
    "                                               special_first=True)\n",
    "        self.vocab.set_default_index(self.UNK_TOKEN)\n",
    "        self.max_sent_len = max_sent_len\n",
    "        self.token_ids = []\n",
    "        self.masked_token = []\n",
    "        self.masked_idx = []\n",
    "        self.is_next = []\n",
    "        self.segment_tokens = []\n",
    "        self._prepare_data()\n",
    "        \n",
    "\n",
    "    def _prepare_data(self) -> None:\n",
    "        for i in range(self.df.shape[0]):\n",
    "            try:\n",
    "                sentences = self.df.iloc[i, 0].split(\". \")\n",
    "                if random.random() <= self.NSP_PERCENTAGE:\n",
    "                    rand_idx = random.randint(0, len(sentences)-2)\n",
    "                    sentences = sentences[rand_idx:rand_idx+2]\n",
    "                    is_next = 0\n",
    "                else:\n",
    "                    rand_idx = random.randint(1, len(sentences)-1)\n",
    "                    sentences = [sentences[rand_idx], sentences[rand_idx-1]]\n",
    "                    is_next = 1\n",
    "                \n",
    "                sentences = [\"[CLS]\"] + self.tokenizer(sentences[0]) + [\"[SEP]\"] + self.tokenizer(sentences[1])\n",
    "                if len(sentences) < self.max_sent_len:\n",
    "                    while len(sentences) < self.max_sent_len:\n",
    "                        sentences += [\"[PAD]\"]\n",
    "                else:\n",
    "                    sentences = sentences[:self.max_sent_len]\n",
    "                \n",
    "                sep_idx = sentences.index(\"[SEP]\")\n",
    "                segment_token = [0]*(sep_idx+1) + [1]*(len(sentences)-1-sep_idx)\n",
    "\n",
    "                # assert len(segment_token) == len(sentences), f\"Length not equal, sep_idx: {sep_idx} \"\n",
    "\n",
    "                token_ids = self.vocab(sentences)\n",
    "                mask_token, mask_idx = self.SEP_TOKEN, -1\n",
    "                while mask_token == self.SEP_TOKEN:\n",
    "                    mask_idx = random.randint(1, len(token_ids)-1)\n",
    "                    mask_token = token_ids[mask_idx]\n",
    "                token_ids[mask_idx] = self.MASK_TOKEN\n",
    "                self.token_ids.append(token_ids)\n",
    "                self.masked_token.append(mask_token)\n",
    "                self.masked_idx.append(mask_idx)\n",
    "                self.segment_tokens.append(segment_token)\n",
    "                self.is_next.append(is_next)\n",
    "            except:\n",
    "                pass\n",
    " \n",
    "        self.bert_df = pd.DataFrame(data={\n",
    "            \"token_ids\" : self.token_ids,\n",
    "            \"segment_tokens\" : self.segment_tokens,\n",
    "            \"masked_token\" : self.masked_token,\n",
    "            \"masked_idx\" : self.masked_idx,\n",
    "            \"is_next\" : self.is_next\n",
    "        })\n",
    "        \n",
    "    def _build_vocab(self, data_iter):\n",
    "        for sentence in data_iter:\n",
    "            yield self.tokenizer(sentence)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        token_ids = self.token_ids[index]\n",
    "        segment_tokens = self.segment_tokens[index]\n",
    "        masked_token = self.masked_token[index]\n",
    "        masked_idx = self.masked_idx[index]\n",
    "        is_next = self.is_next[index]\n",
    "        return torch.tensor(token_ids), torch.tensor(segment_tokens), torch.tensor(masked_token), torch.tensor(masked_idx), torch.tensor(is_next)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.bert_df.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_ids</th>\n",
       "      <th>segment_tokens</th>\n",
       "      <th>masked_token</th>\n",
       "      <th>masked_idx</th>\n",
       "      <th>is_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 925, 12, 4235, 6, 13, 5, 399, 375, 10, 5, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1263</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 151, 4726, 381, 53, 40, 10, 5, 106, 1698, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2231</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 28, 133, 12, 4391, 6, 29, 5, 448, 12, 2044...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 10662, 35, 35401, 206, 7484, 50, 13, 174, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
       "      <td>4532</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 1371, 13291, 6, 11658, 5504, 6, 3952, 3077...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>524</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41933</th>\n",
       "      <td>[0, 159, 12, 85, 139, 168, 1, 111, 113, 21, 25...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>639</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41934</th>\n",
       "      <td>[0, 5, 64, 157, 123, 53, 15, 5, 64, 157, 50, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>186</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41935</th>\n",
       "      <td>[0, 1619, 133, 6, 101, 448, 6, 101, 135, 6, 32...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41936</th>\n",
       "      <td>[0, 59, 66, 50, 11, 39, 13, 5, 3207, 10, 17, 1...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>12178</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41937</th>\n",
       "      <td>[0, 893, 6, 19, 24, 77, 8, 5540, 6, 4118, 133,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41938 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               token_ids  \\\n",
       "0      [0, 925, 12, 4235, 6, 13, 5, 399, 375, 10, 5, ...   \n",
       "1      [0, 151, 4726, 381, 53, 40, 10, 5, 106, 1698, ...   \n",
       "2      [0, 28, 133, 12, 4391, 6, 29, 5, 448, 12, 2044...   \n",
       "3      [0, 10662, 35, 35401, 206, 7484, 50, 13, 174, ...   \n",
       "4      [0, 1371, 13291, 6, 11658, 5504, 6, 3952, 3077...   \n",
       "...                                                  ...   \n",
       "41933  [0, 159, 12, 85, 139, 168, 1, 111, 113, 21, 25...   \n",
       "41934  [0, 5, 64, 157, 123, 53, 15, 5, 64, 157, 50, 1...   \n",
       "41935  [0, 1619, 133, 6, 101, 448, 6, 101, 135, 6, 32...   \n",
       "41936  [0, 59, 66, 50, 11, 39, 13, 5, 3207, 10, 17, 1...   \n",
       "41937  [0, 893, 6, 19, 24, 77, 8, 5540, 6, 4118, 133,...   \n",
       "\n",
       "                                          segment_tokens  masked_token  \\\n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          1263   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          2231   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             5   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...          4532   \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...           524   \n",
       "...                                                  ...           ...   \n",
       "41933  [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...           639   \n",
       "41934  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...           186   \n",
       "41935  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             6   \n",
       "41936  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...         12178   \n",
       "41937  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            10   \n",
       "\n",
       "       masked_idx  is_next  \n",
       "0              36        0  \n",
       "1              41        0  \n",
       "2              18        1  \n",
       "3              21        0  \n",
       "4              10        1  \n",
       "...           ...      ...  \n",
       "41933          12        1  \n",
       "41934          22        1  \n",
       "41935          25        0  \n",
       "41936          24        1  \n",
       "41937          23        0  \n",
       "\n",
       "[41938 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = IMDBBERTDataset(\"../data/IMDB.csv\")\n",
    "ds.bert_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89854"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(ds.vocab)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int=512,\n",
    "                 vocab_size: int=1000,\n",
    "                 max_seq_len: int=100,\n",
    "                 dropout: float=0.1) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                            embedding_dim=d_model)\n",
    "        self.pe = torch.zeros(size=(max_seq_len, d_model),\n",
    "                              requires_grad=False)\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for dim in range(d_model):\n",
    "                if pos%2==0:\n",
    "                    self.pe[pos, dim] = math.sin(pos//(10000**(2*dim//d_model)))\n",
    "                else:\n",
    "                    self.pe[pos, dim] = math.cos(pos//(10000**(2*dim//d_model)))\n",
    "\n",
    "        self.segment_embedding = nn.Embedding(num_embeddings=2,\n",
    "                                              embedding_dim=d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"BERTEmbedding(d_model={self.d_model}, vocab_size={self.vocab_size})\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"BERTEmbedding(d_model={self.d_model}, vocab_size={self.vocab_size})\"\n",
    "\n",
    "    def forward(self, \n",
    "                x: torch.Tensor,\n",
    "                segment_tokens: torch.Tensor) -> torch.Tensor:\n",
    "        # x -> [batch_size, max_seq_len]\n",
    "        token_embeddings = self.token_embedding(x)\n",
    "        position_encoding = self.pe[:x.shape[1], :].unsqueeze(0) # positional_encoding -> [1, max_seq_len, d_model]\n",
    "        segment_embedding = self.segment_embedding(segment_tokens)\n",
    "        return self.dropout(token_embeddings + position_encoding + segment_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids shape: torch.Size([32, 50])\n",
      "embedding shape: torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_loader = DataLoader(dataset=ds,\n",
    "                         batch_size=32,\n",
    "                         shuffle=True)\n",
    "batch = next(iter(data_loader))\n",
    "bert_embedding = BERTEmbedding(d_model=512,\n",
    "                               vocab_size=VOCAB_SIZE)\n",
    "with torch.inference_mode():\n",
    "    token_ids, segment_tokens = batch[0], batch[1]\n",
    "    embedding = bert_embedding(token_ids, segment_tokens)\n",
    "    print(f\"token_ids shape: {token_ids.shape}\")\n",
    "    print(f\"embedding shape: {embedding.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BERT model is made up of just the encoder part of the transformer architecture.\n",
    "* I am going to build the BERT-Base model in this kernel\n",
    "* Parameters for BERT-Base: \n",
    "    * Number of encoder layers   **L : 12**\n",
    "    * Model Dimension            **H : 768**\n",
    "    * Number of Attention heads  **A : 12** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 768])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.rand(32, 50, 768)\n",
    "masked_idx = torch.tensor([random.randint(0, 49) for _ in range(32)])\n",
    "masked_tokens = batch[range(len(masked_idx)), masked_idx]\n",
    "masked_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.scripts import TransformerEncoder, create_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers: int=12,\n",
    "                 d_model: int=768,\n",
    "                 num_heads: int=12,\n",
    "                 vocab_size: int=1000,\n",
    "                 d_ff: int=2048,\n",
    "                 attn_dropout: float=0.1,\n",
    "                 ff_dropout: float=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.bert_encoder = TransformerEncoder(num_encoders=num_layers,\n",
    "                                               d_model=d_model,\n",
    "                                               num_heads=num_heads,\n",
    "                                               d_ff=d_ff,\n",
    "                                               attn_dropout=attn_dropout,\n",
    "                                               ff_dropout=ff_dropout)\n",
    "        self.masked_block = nn.Linear(in_features=d_model,\n",
    "                                      out_features=vocab_size)\n",
    "        self.nsp_block = nn.Linear(in_features=d_model,\n",
    "                                        out_features=2)\n",
    "    \n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"BERT(num_layers={self.num_layers}, d_model={self.d_model}, num_heads={self.num_heads})\"\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"BERT(num_layers={self.num_layers}, d_model={self.d_model}, num_heads={self.num_heads})\"\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                mask: torch.Tensor,\n",
    "                masked_idx: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        \"\"\"\n",
    "            x -> Input embedding, shape: [batch_size, max_seq_len, d_model]\n",
    "            mask -> Mask for padding, shape: [batch_size, 1, 1, max_seq_len]\n",
    "            masked_idx -> For each sequence a different index position has been masked\n",
    "            and the encoder's contextual representation of the masked token will be\n",
    "            used to predict the true token. To extract the representation for each \n",
    "            sequence the different index positions are passed in masked_idx which will \n",
    "            be used for indexing the representation. Shape: [batch_size]\n",
    "        \"\"\"\n",
    "        x = self.bert_encoder(x, mask) # [batch_size, max_seq_len, d_model]\n",
    "        masked_tokens = x[range(len(masked_idx)), masked_idx]\n",
    "\n",
    "        # For NSP prediction the BERT paper uses the '[CLS]' token which is the \n",
    "        # 0th index in each sequence and it is accessed by indexing '0' along the\n",
    "        # first dimension\n",
    "        nsp_logits = self.nsp_block(x[:, 0, :]) # x[:, 0, :] -> [batch_size, d_model]\n",
    "        masked_tokens_logits = self.masked_block(masked_tokens)\n",
    "        return masked_tokens_logits, nsp_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(bert_model: BERT,\n",
    "               bert_embedding: BERTEmbedding,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               masked_loss_fn: torch.nn.Module,\n",
    "               nsp_loss_fn: torch.nn.Module,\n",
    "               epochs: int=10,\n",
    "               device: str=\"cpu\") -> Tuple[float, float]:\n",
    "    \n",
    "    bert_model.to(device)\n",
    "    bert_embedding.to(device)\n",
    "    bert_model.train()\n",
    "\n",
    "    bert_results = {\n",
    "        \"masked_tokens_losses\" : [],\n",
    "        \"nsp_losses\" : [],\n",
    "        \"masked_tokens_acc\" : [],\n",
    "        \"nsp_acc\" : [],\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        masked_tokens_losses, nsp_losses = 0, 0\n",
    "        masked_token_acc, nsp_acc = 0, 0\n",
    "        for token_ids, segment_tokens, masked_token, masked_idx, is_next in tqdm(dataloader):\n",
    "            \n",
    "            token_ids, segment_tokens = token_ids.to(device), segment_tokens.to(device)\n",
    "            masked_tokens, masked_idx = masked_token.to(device), masked_idx.to(device)\n",
    "            is_next = is_next.to(device)\n",
    "            \n",
    "            mask = create_padding_mask(token_ids, PAD_TOKEN)\n",
    "            tokens_embedded = bert_embedding(token_ids, segment_tokens)\n",
    "            masked_tokens_logits, nsp_logits = bert_model(tokens_embedded, mask, masked_idx)\n",
    "\n",
    "            masked_tokens_logits = torch.softmax(masked_tokens_logits, dim=-1)\n",
    "            nsp_logits = torch.softmax(nsp_logits, dim=-1)\n",
    "\n",
    "            masked_tokens_loss = masked_loss_fn(masked_tokens_logits, masked_tokens)\n",
    "            nsp_loss = nsp_loss_fn(nsp_logits, is_next)\n",
    "            total_loss = masked_tokens_loss + nsp_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            masked_tokens_losses += masked_tokens_loss.item()\n",
    "            nsp_losses += nsp_loss.item()\n",
    "            masked_token_acc += (masked_tokens_logits.argmax(dim=1).squeeze() == masked_token).sum()\n",
    "            nsp_acc += (nsp_logits.argmax(dim=1).squeeze() == is_next).sum()\n",
    "        \n",
    "        masked_tokens_losses = masked_tokens_losses/BATCH_SIZE\n",
    "        nsp_losses = nsp_losses/BATCH_SIZE\n",
    "        masked_token_acc = masked_token_acc/BATCH_SIZE\n",
    "        nsp_acc = nsp_acc/BATCH_SIZE\n",
    "\n",
    "        bert_results[\"masked_tokens_acc\"].append(masked_token_acc)\n",
    "        bert_results[\"masked_tokens_losses\"].append(masked_tokens_losses)\n",
    "        bert_results[\"nsp_losses\"].append(nsp_losses)\n",
    "        bert_results[\"nsp_acc\"].append(nsp_acc)\n",
    "\n",
    "        print(f\"epoch: {epoch+1} masked_tokens_losses: {masked_tokens_losses:.3f} | masked_token_acc: {masked_token_acc:.3f} | nsp_losses: {nsp_losses:.3f} | nsp_acc: {nsp_acc:.3f}\")\n",
    "    \n",
    "    return bert_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = BERT(num_layers=12, d_model=768, num_heads=12, vocab_size=VOCAB_SIZE)\n",
    "bert_embedding = BERTEmbedding(d_model=768, vocab_size=VOCAB_SIZE)\n",
    "dataloader = torch.utils.data.DataLoader(dataset=IMDBBERTDataset(path=\"../data/IMDB.csv\"),\n",
    "                                         shuffle=True,\n",
    "                                         batch_size=BATCH_SIZE)\n",
    "optimizer = torch.optim.Adam(params=bert_model.parameters(),\n",
    "                             lr=1e-4,\n",
    "                             betas=(0.9, 0.999),\n",
    "                             weight_decay=0.01)\n",
    "masked_loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "nsp_loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results = train_bert(bert_model=bert_model,\n",
    "                          bert_embedding=bert_embedding,\n",
    "                          dataloader=dataloader,\n",
    "                          optimizer=optimizer,\n",
    "                          masked_loss_fn=masked_loss_fn,\n",
    "                          nsp_loss_fn=nsp_loss_fn,\n",
    "                          epochs=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
