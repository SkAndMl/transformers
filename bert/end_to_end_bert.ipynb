{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/IMDB.csv\")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASK_PERCENTANGE = 0.15\n",
    "MASK_PROBABILITY = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked\n",
      "My husband received DVD of OBWAT for Christmas and it was the best gift we received! We watch it every time we need to laugh and so far we have viewed it 12 times!The scenery in this movie is beautiful and the music is outstanding!We also purchased the soundtrack and we play it in our vehicles and at home when ever we need a pick me up and that too is daily!If anyone needs a suggestion for a good gift for movie lovers this movie is it!The characters are hilarious , charming , and their facial expressions are too funny to [MASK] have always been a fan of George Clooney but now I am also a fan of Tim Blake Nelson(Delmar ) and John Turturro (Pete)and am now looking for them in other movies! You gotta see this movie!!!\n",
      "NSP: 1\n",
      "My husband received DVD of OBWAT for Christmas and it was the best gift we received! We watch it every time we need to laugh and so far we have viewed it 12 times!The scenery in this movie is beautiful and the music is outstanding!We also purchased the soundtrack and we play it in our vehicles and at home when ever we need a pick me up and that too is daily!If anyone needs a suggestion for a good gift for movie lovers this movie is it!The characters are hilarious , charming , and their facial expressions are too funny to describe!I have always been a fan of George Clooney but now I am also a fan of Tim Blake Nelson(Delmar ) and John Turturro (Pete)and am now looking for them in other movies! You gotta see this movie!!!\n",
      "139\n"
     ]
    }
   ],
   "source": [
    "sents = df.iloc[random.randint(0, df.shape[0]), 0].split(\". \")\n",
    "idx = random.randint(0, len(sents)-1)\n",
    "sent1 = sents[idx]\n",
    "\n",
    "sent1 = sent1.split(\" \")\n",
    "mask_idx = random.randint(0, len(sent1)-1)\n",
    "if random.random() < MASK_PROBABILITY:\n",
    "    sent1[mask_idx] = \"[MASK]\"\n",
    "    print(\"Masked\")\n",
    "    print(\" \".join(sent1))\n",
    "else:\n",
    "    rand_token = random.randint(0, len(sent1)-1)\n",
    "    sent1[mask_idx] = sent1[rand_token]\n",
    "    print(\"Replaced with random token\")\n",
    "    print(\" \".join(sent1))\n",
    "\n",
    "if random.random() <= 0.5:\n",
    "    nsp_sents = \". [SEP] \".join(sents[idx: idx+2])\n",
    "    print(\"NSP: 1\")\n",
    "    print(nsp_sents)\n",
    "    print(len(nsp_sents.split(\" \")))\n",
    "else:\n",
    "    nsp_sents = sents[idx] + \". [SEP] \" + sents[random.randint(idx+1, len(sents))-1]\n",
    "    print(\"NSP: 0\")\n",
    "    print(nsp_sents)\n",
    "    print(len(nsp_sents.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLS_TOKEN = 0\n",
    "SEP_TOKEN = 1\n",
    "MASK_TOKEN = 2\n",
    "PAD_TOKEN = 3 \n",
    "UNK_TOKEN = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "\n",
    "def build_vocab(data_iter):\n",
    "    for sentence in data_iter:\n",
    "        yield tokenizer(sentence)\n",
    "\n",
    "vocab = build_vocab_from_iterator(build_vocab(df[\"review\"].to_list()),\n",
    "                                  min_freq=2,\n",
    "                                  specials=[\"[CLS]\", \"[SEP]\", \"[MASK]\", \"[PAD]\", \"<UNK>\"],\n",
    "                                  special_first=True)\n",
    "vocab.set_default_index(UNK_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89854"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4995712, 5004288)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "less_ = 0\n",
    "more_ = 0\n",
    "for _ in range(10000000):\n",
    "    if random.random()<=0.5:\n",
    "        less_ += 1\n",
    "    else:\n",
    "        more_ += 1\n",
    "less_, more_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`random.random()` generates an equal distribution and it can be used for the NSP task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBBERTDataset(Dataset):\n",
    "\n",
    "    NSP_PERCENTAGE = 0.50\n",
    "\n",
    "    CLS_TOKEN = 0\n",
    "    SEP_TOKEN = 1\n",
    "    MASK_TOKEN = 2\n",
    "    PAD_TOKEN = 3 \n",
    "    UNK_TOKEN = 4\n",
    "\n",
    "    def __init__(self,\n",
    "                 path: str,\n",
    "                 max_sent_len: int=50) -> None:\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(path)\n",
    "        self.tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "        self.vocab = build_vocab_from_iterator(self._build_vocab(self.df[\"review\"].to_list()),\n",
    "                                               min_freq=2,\n",
    "                                               specials=[\"[CLS]\", \"[SEP]\", \"[MASK]\", \"[PAD]\", \"<UNK>\"],\n",
    "                                               special_first=True)\n",
    "        self.vocab.set_default_index(self.UNK_TOKEN)\n",
    "        self.max_sent_len = max_sent_len\n",
    "        self.token_ids = []\n",
    "        self.masked_token = []\n",
    "        self.masked_idx = []\n",
    "        self.is_next = []\n",
    "        self.segment_tokens = []\n",
    "        self._prepare_data()\n",
    "        \n",
    "\n",
    "    def _prepare_data(self) -> None:\n",
    "        for i in range(self.df.shape[0]):\n",
    "            try:\n",
    "                sentences = self.df.iloc[i, 0].split(\". \")\n",
    "                if random.random() <= self.NSP_PERCENTAGE:\n",
    "                    rand_idx = random.randint(0, len(sentences)-2)\n",
    "                    sentences = sentences[rand_idx:rand_idx+2]\n",
    "                    is_next = 0\n",
    "                else:\n",
    "                    rand_idx = random.randint(1, len(sentences)-1)\n",
    "                    sentences = [sentences[rand_idx], sentences[rand_idx-1]]\n",
    "                    is_next = 1\n",
    "                \n",
    "                sentences = [\"[CLS]\"] + self.tokenizer(sentences[0]) + [\"[SEP]\"] + self.tokenizer(sentences[1])\n",
    "                if len(sentences) < self.max_sent_len:\n",
    "                    while len(sentences) < self.max_sent_len:\n",
    "                        sentences += [\"[PAD]\"]\n",
    "                else:\n",
    "                    sentences = sentences[:self.max_sent_len]\n",
    "                \n",
    "                sep_idx = sentences.index(\"[SEP]\")\n",
    "                segment_token = [0]*(sep_idx+1) + [1]*(len(sentences)-1-sep_idx)\n",
    "\n",
    "                # assert len(segment_token) == len(sentences), f\"Length not equal, sep_idx: {sep_idx} \"\n",
    "\n",
    "                token_ids = self.vocab(sentences)\n",
    "                mask_token, mask_idx = self.SEP_TOKEN, -1\n",
    "                while mask_token == self.SEP_TOKEN:\n",
    "                    mask_idx = random.randint(1, len(token_ids)-1)\n",
    "                    mask_token = token_ids[mask_idx]\n",
    "                token_ids[mask_idx] = self.MASK_TOKEN\n",
    "                self.token_ids.append(token_ids)\n",
    "                self.masked_token.append(mask_token)\n",
    "                self.masked_idx.append(mask_idx)\n",
    "                self.segment_tokens.append(segment_token)\n",
    "                self.is_next.append(is_next)\n",
    "            except:\n",
    "                pass\n",
    " \n",
    "        self.bert_df = pd.DataFrame(data={\n",
    "            \"token_ids\" : self.token_ids,\n",
    "            \"segment_tokens\" : self.segment_tokens,\n",
    "            \"masked_token\" : self.masked_token,\n",
    "            \"masked_idx\" : self.masked_idx,\n",
    "            \"is_next\" : self.is_next\n",
    "        })\n",
    "        \n",
    "    def _build_vocab(self, data_iter):\n",
    "        for sentence in data_iter:\n",
    "            yield self.tokenizer(sentence)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        token_ids = self.token_ids[index]\n",
    "        segment_tokens = self.segment_tokens[index]\n",
    "        masked_token = self.masked_token[index]\n",
    "        masked_idx = self.masked_idx[index]\n",
    "        is_next = self.is_next[index]\n",
    "        return torch.tensor(token_ids), torch.tensor(segment_tokens), torch.tensor(masked_token), torch.tensor(masked_idx), torch.tensor(is_next)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.bert_df.shape[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT EMBEDDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_ids</th>\n",
       "      <th>segment_tokens</th>\n",
       "      <th>masked_token</th>\n",
       "      <th>masked_idx</th>\n",
       "      <th>is_next</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0, 28, 109, 445, 14, 148, 236, 3404, 87, 23, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>15</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0, 28, 133, 12, 4391, 6, 29, 5, 448, 12, 2044...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[0, 363, 35, 638, 11, 79, 115, 170, 103, 475, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[0, 28, 383, 35, 6, 23, 10720, 579, 6, 64, 50,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>275</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[0, 69, 140, 22, 46, 549, 6, 1512, 206, 4470, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42011</th>\n",
       "      <td>[0, 59, 22, 8, 242, 35704, 275, 1, 5, 64, 157,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42012</th>\n",
       "      <td>[0, 6805, 87, 6, 105, 14, 1613, 19, 3216, 55, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>144</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42013</th>\n",
       "      <td>[0, 14, 2, 8, 4212, 4674, 13, 49834, 9209, 633...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>263</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42014</th>\n",
       "      <td>[0, 69, 12, 8, 391, 993, 6, 13181, 4418, 2031,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>10</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42015</th>\n",
       "      <td>[0, 453, 40, 6330, 5, 1157, 2, 117, 11, 39, 35...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>2495</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42016 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               token_ids  \\\n",
       "0      [0, 28, 109, 445, 14, 148, 236, 3404, 87, 23, ...   \n",
       "1      [0, 28, 133, 12, 4391, 6, 29, 5, 448, 12, 2044...   \n",
       "2      [0, 363, 35, 638, 11, 79, 115, 170, 103, 475, ...   \n",
       "3      [0, 28, 383, 35, 6, 23, 10720, 579, 6, 64, 50,...   \n",
       "4      [0, 69, 140, 22, 46, 549, 6, 1512, 206, 4470, ...   \n",
       "...                                                  ...   \n",
       "42011  [0, 59, 22, 8, 242, 35704, 275, 1, 5, 64, 157,...   \n",
       "42012  [0, 6805, 87, 6, 105, 14, 1613, 19, 3216, 55, ...   \n",
       "42013  [0, 14, 2, 8, 4212, 4674, 13, 49834, 9209, 633...   \n",
       "42014  [0, 69, 12, 8, 391, 993, 6, 13181, 4418, 2031,...   \n",
       "42015  [0, 453, 40, 6330, 5, 1157, 2, 117, 11, 39, 35...   \n",
       "\n",
       "                                          segment_tokens  masked_token  \\\n",
       "0      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            15   \n",
       "1      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             5   \n",
       "2      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            54   \n",
       "3      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...           275   \n",
       "4      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...             3   \n",
       "...                                                  ...           ...   \n",
       "42011  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, ...             3   \n",
       "42012  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...           144   \n",
       "42013  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...           263   \n",
       "42014  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...            10   \n",
       "42015  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...          2495   \n",
       "\n",
       "       masked_idx  is_next  \n",
       "0              24        1  \n",
       "1              12        0  \n",
       "2              47        1  \n",
       "3              26        0  \n",
       "4              41        0  \n",
       "...           ...      ...  \n",
       "42011          45        0  \n",
       "42012          34        1  \n",
       "42013           2        0  \n",
       "42014          34        0  \n",
       "42015           6        0  \n",
       "\n",
       "[42016 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = IMDBBERTDataset(\"../data/IMDB.csv\")\n",
    "ds.bert_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89854"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE = len(ds.vocab)\n",
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 d_model: int=512,\n",
    "                 vocab_size: int=1000,\n",
    "                 max_seq_len: int=100,\n",
    "                 dropout: float=0.1) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                            embedding_dim=d_model)\n",
    "        self.pe = torch.zeros(size=(max_seq_len, d_model),\n",
    "                              requires_grad=False)\n",
    "        \n",
    "        for pos in range(max_seq_len):\n",
    "            for dim in range(d_model):\n",
    "                if pos%2==0:\n",
    "                    self.pe[pos, dim] = math.sin(pos//(10000**(2*dim//d_model)))\n",
    "                else:\n",
    "                    self.pe[pos, dim] = math.cos(pos//(10000**(2*dim//d_model)))\n",
    "\n",
    "        self.segment_embedding = nn.Embedding(num_embeddings=2,\n",
    "                                              embedding_dim=d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"BERTEmbedding(d_model={self.d_model}, vocab_size={self.vocab_size})\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"BERTEmbedding(d_model={self.d_model}, vocab_size={self.vocab_size})\"\n",
    "\n",
    "    def forward(self, \n",
    "                x: torch.Tensor,\n",
    "                segment_tokens: torch.Tensor) -> torch.Tensor:\n",
    "        # x -> [batch_size, max_seq_len]\n",
    "        token_embeddings = self.token_embedding(x)\n",
    "        position_encoding = self.pe[:x.shape[1], :].unsqueeze(0) # positional_encoding -> [1, max_seq_len, d_model]\n",
    "        segment_embedding = self.segment_embedding(segment_tokens)\n",
    "        return self.dropout(token_embeddings + position_encoding + segment_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids shape: torch.Size([32, 50])\n",
      "embedding shape: torch.Size([32, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "data_loader = DataLoader(dataset=ds,\n",
    "                         batch_size=32,\n",
    "                         shuffle=True)\n",
    "batch = next(iter(data_loader))\n",
    "bert_embedding = BERTEmbedding(d_model=512,\n",
    "                               vocab_size=VOCAB_SIZE)\n",
    "with torch.inference_mode():\n",
    "    token_ids, segment_tokens = batch[0], batch[1]\n",
    "    embedding = bert_embedding(token_ids, segment_tokens)\n",
    "    print(f\"token_ids shape: {token_ids.shape}\")\n",
    "    print(f\"embedding shape: {embedding.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* BERT model is made up of just the encoder part of the transformer architecture.\n",
    "* I am going to build the BERT-Base model in this kernel\n",
    "* Parameters for BERT-Base: \n",
    "    * Number of encoder layers   **L : 12**\n",
    "    * Model Dimension            **H : 768**\n",
    "    * Number of Attention heads  **A : 12** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 768])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = torch.rand(32, 50, 768)\n",
    "masked_idx = torch.tensor([random.randint(0, 49) for _ in range(32)])\n",
    "masked_tokens = batch[range(len(masked_idx)), masked_idx]\n",
    "masked_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.scripts import TransformerEncoder, create_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_layers: int=12,\n",
    "                 d_model: int=768,\n",
    "                 num_heads: int=12,\n",
    "                 vocab_size: int=1000,\n",
    "                 d_ff: int=2048,\n",
    "                 attn_dropout: float=0.1,\n",
    "                 ff_dropout: float=0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.bert_encoder = TransformerEncoder(num_encoders=num_layers,\n",
    "                                               d_model=d_model,\n",
    "                                               num_heads=num_heads,\n",
    "                                               d_ff=d_ff,\n",
    "                                               attn_dropout=attn_dropout,\n",
    "                                               ff_dropout=ff_dropout)\n",
    "        self.masked_block = nn.Linear(in_features=d_model,\n",
    "                                      out_features=vocab_size)\n",
    "        self.nsp_block = nn.Linear(in_features=d_model,\n",
    "                                        out_features=2)\n",
    "    \n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"BERT(num_layers={self.num_layers}, d_model={self.d_model}, num_heads={self.num_heads})\"\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"BERT(num_layers={self.num_layers}, d_model={self.d_model}, num_heads={self.num_heads})\"\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                mask: torch.Tensor,\n",
    "                masked_idx: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        \"\"\"\n",
    "            x -> Input embedding, shape: [batch_size, max_seq_len, d_model]\n",
    "            mask -> Mask for padding, shape: [batch_size, 1, 1, max_seq_len]\n",
    "            masked_idx -> For each sequence a different index position has been masked\n",
    "            and the encoder's contextual representation of the masked token will be\n",
    "            used to predict the true token. To extract the representation for each \n",
    "            sequence the different index positions are passed in masked_idx which will \n",
    "            be used for indexing the representation. Shape: [batch_size]\n",
    "        \"\"\"\n",
    "        x = self.bert_encoder(x, mask) # [batch_size, max_seq_len, d_model]\n",
    "        masked_tokens = x[range(len(masked_idx)), masked_idx]\n",
    "\n",
    "        # For NSP prediction the BERT paper uses the '[CLS]' token which is the \n",
    "        # 0th index in each sequence and it is accessed by indexing '0' along the\n",
    "        # first dimension\n",
    "        nsp_logits = self.nsp_block(x[:, 0, :]) # x[:, 0, :] -> [batch_size, d_model]\n",
    "        masked_tokens_logits = self.masked_block(masked_tokens)\n",
    "        return masked_tokens_logits, nsp_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bert(bert_model: BERT,\n",
    "               bert_embedding: BERTEmbedding,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               optimizer: torch.optim.Optimizer,\n",
    "               masked_loss_fn: torch.nn.Module,\n",
    "               nsp_loss_fn: torch.nn.Module,\n",
    "               epochs: int=10,\n",
    "               device: str=\"cpu\") -> Tuple[float, float]:\n",
    "    \n",
    "    bert_model.to(device)\n",
    "    bert_embedding.to(device)\n",
    "    bert_model.train()\n",
    "\n",
    "    bert_results = {\n",
    "        \"masked_tokens_losses\" : [],\n",
    "        \"nsp_losses\" : [],\n",
    "        \"masked_tokens_acc\" : 0,\n",
    "        \"nsp_acc\" : 0,\n",
    "    }\n",
    "\n",
    "    for epoch in tqdm(epochs):\n",
    "        for token_ids, segment_tokens, masked_token, masked_idx, is_next in tqdm(dataloader):\n",
    "            \n",
    "            token_ids, segment_tokens = token_ids.to(device), segment_tokens.to(device)\n",
    "            masked_tokens, masked_idx = masked_token.to(device), masked_idx.to(device)\n",
    "            is_next = is_next.to(device)\n",
    "            \n",
    "            mask = create_padding_mask(token_ids)\n",
    "            tokens_embedded = bert_embedding(token_ids, segment_tokens)\n",
    "            masked_tokens_logits, nsp_logits = bert_model(tokens_embedded, mask)\n",
    "            masked_tokens_loss = masked_loss_fn(masked_tokens_logits, masked_tokens)\n",
    "            nsp_loss = nsp_loss_fn(nsp_logits, is_next)\n",
    "            total_loss = masked_tokens_loss + nsp_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
