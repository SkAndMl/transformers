{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import random\n",
    "import math\n",
    "from typing import Tuple, List\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/IMDB.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the MaskedLM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDBMaskedBertDataset(Dataset):\n",
    "\n",
    "    PAD_TOKEN = 0\n",
    "    CLS_TOKEN = 1\n",
    "    SEP_TOKEN = 2\n",
    "    MASK_TOKEN = 3\n",
    "    UNK_TOKEN = 4\n",
    "\n",
    "    SPECIALS = [\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"]\n",
    "\n",
    "    def __init__(self,\n",
    "                 path: str,\n",
    "                 max_len: int=10) -> None:\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.df = pd.read_csv(path)\n",
    "\n",
    "        self.tokenizer = get_tokenizer(tokenizer=\"spacy\",\n",
    "                                       language=\"en_core_web_sm\")\n",
    "\n",
    "        self.masked_tokens = []\n",
    "        self.masked_token_idxs = []\n",
    "        self.sentences = []\n",
    "\n",
    "        self._prepare_data()\n",
    "        self.sentences = torch.tensor(self.sentences)\n",
    "        self.masked_token_idxs = torch.tensor(self.masked_token_idxs)\n",
    "        self.masked_tokens = torch.tensor(self.masked_tokens).squeeze()\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        return self.sentences[index], self.masked_tokens[index], self.masked_token_idxs[index]\n",
    "    \n",
    "    def _build_vocab(self, data_iter):\n",
    "        for sent in data_iter:\n",
    "            yield self.tokenizer(sent)\n",
    "\n",
    "\n",
    "    def _prepare_data(self):\n",
    "\n",
    "        for i in range(self.df.shape[0]):\n",
    "            for sent in self.df.iloc[i, 0].split('. '):\n",
    "                if 1 < len(self.tokenizer(sent)) <= self.max_len:\n",
    "                    self.sentences.append(sent)\n",
    "        \n",
    "        self.vocab = build_vocab_from_iterator(self._build_vocab(self.sentences),\n",
    "                                               min_freq=2,\n",
    "                                               special_first=True,\n",
    "                                               specials=self.SPECIALS)\n",
    "        self.vocab.set_default_index(self.UNK_TOKEN)\n",
    "        \n",
    "        self._mask_data()\n",
    "\n",
    "        for i in range(len(self.sentences)):\n",
    "            self.sentences[i] = self.vocab(self.sentences[i])\n",
    "            self.masked_tokens[i] = self.vocab(self.masked_tokens[i])\n",
    "\n",
    "    \n",
    "    def _mask_data(self):\n",
    "\n",
    "        for i in range(len(self.sentences)):\n",
    "            sentence = self.tokenizer(self.sentences[i])\n",
    "            mask_idx = random.randint(0, len(sentence)-1)\n",
    "            self.masked_token_idxs.append(mask_idx+1)\n",
    "            self.masked_tokens.append([sentence[mask_idx]])\n",
    "            sentence[mask_idx] = \"[MASK]\"\n",
    "            sentence = [\"[CLS]\"] + sentence + [\"[SEP]\"]\n",
    "            while len(sentence)<self.max_len+2:\n",
    "                sentence.append(\"[PAD]\")\n",
    "            self.sentences[i] = sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_ds = IMDBMaskedBertDataset(path=\"../data/IMDB.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  1,   3,  51,  26, 167,  16,  34,  29,   2,   0,   0,   0]),\n",
       " tensor(8),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_ds[random.randint(0, len(masked_ds)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([89172, 12]), torch.Size([89172]), torch.Size([89172]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_ds.sentences.shape, masked_ds.masked_tokens.shape, masked_ds.masked_token_idxs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "VOCAB_SIZE = len(masked_ds.vocab)\n",
    "D_MODEL = 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(dataset=masked_ds,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the `Next Sentence Prediction` is not carried out in this notebook, we need not implement `Segment Embedding` for Masked LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 max_len: int=10,\n",
    "                 vocab_size: int=1000,\n",
    "                 d_model: int=768,\n",
    "                 dropout_prob: float=0.1)-> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.max_len = max_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size,\n",
    "                                      embedding_dim=d_model)\n",
    "        \n",
    "\n",
    "        pe = torch.zeros(size=(max_len+2, d_model))\n",
    "        for pos in range(self.max_len):\n",
    "            for i in range(self.d_model):\n",
    "                if i%2==0:\n",
    "                    pe[pos, i] = math.sin(pos/10000**(2*i/d_model))\n",
    "                else:\n",
    "                    pe[pos, i] = math.cos(pos/10000**(2*i/d_model))\n",
    "        \n",
    "        self.register_buffer(\"pe\", pe)\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Embedding(max_len={self.max_len}, vocab_size={self.vocab_size}, d_model={self.d_model}, dropout_prob={self.dropout_prob})\"\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Embedding(max_len={self.max_len}, vocab_size={self.vocab_size}, d_model={self.d_model}, dropout_prob={self.dropout_prob})\"\n",
    "    \n",
    "    def forward(self, \n",
    "                x: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        embedded = self.pe + self.embedding(x)\n",
    "        return self.dropout(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "embedding = Embedding(vocab_size=VOCAB_SIZE,\n",
    "                      d_model=D_MODEL)\n",
    "with torch.inference_mode():\n",
    "    print(embedding(masked_ds[0][0].unsqueeze(0)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT model for MaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"..\")\n",
    "from scripts.scripts import TransformerEncoder, create_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTMaskedLM(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_encoders: int=12,\n",
    "                 d_model: int=768,\n",
    "                 num_heads: int=12,\n",
    "                 d_ff: int=3072,\n",
    "                 attn_dropout_prob: float=0.1,\n",
    "                 ff_dropout_prob: float=0.1,\n",
    "                 output_size: int=1000) -> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.encoder_stack = TransformerEncoder(num_encoders=num_encoders,\n",
    "                                                d_model=d_model,\n",
    "                                                num_heads=num_heads,\n",
    "                                                d_ff=d_ff,\n",
    "                                                attn_dropout=attn_dropout_prob,\n",
    "                                                ff_dropout=ff_dropout_prob)\n",
    "        self.classification_head = nn.Linear(in_features=d_model,\n",
    "                                             out_features=output_size)\n",
    "    \n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                mask: torch.Tensor,\n",
    "                masked_token_idxs: torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "        encoder_output = self.encoder_stack(x, mask)\n",
    "        masked_tokens_context = encoder_output[range(len(masked_token_idxs)), masked_token_idxs]\n",
    "        return self.classification_head(masked_tokens_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "CLS_TOKEN = 1\n",
    "SEP_TOKEN = 2\n",
    "MASK_TOKEN = 3\n",
    "UNK_TOKEN = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_masked_lm(bert: BERTMaskedLM,\n",
    "                    bert_embedding: Embedding,\n",
    "                    data_loader: DataLoader,\n",
    "                    loss_fn: nn.Module,\n",
    "                    optimizer: torch.optim.Optimizer,\n",
    "                    epochs: int=2,\n",
    "                    device: torch.device=\"cpu\") -> Tuple[List[int], List[int]]:\n",
    "    \n",
    "    losses = []\n",
    "    accs = []\n",
    "    bert = bert.to(device)\n",
    "    bert_embedding = bert_embedding.to(device)\n",
    "    bert.train()\n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        for batch, (sentence, masked_token, masked_token_idx) in enumerate(data_loader):\n",
    "            sentence = sentence.to(device)\n",
    "            masked_token = masked_token.to(device)\n",
    "            masked_token_idx = masked_token_idx.to(device)\n",
    "            \n",
    "            mask = create_padding_mask(sentence, padding_idx=PAD_TOKEN)\n",
    "            input_embedding = bert_embedding(sentence)\n",
    "            logits = bert(input_embedding, mask, masked_token_idx)\n",
    "\n",
    "            loss = loss_fn(logits.squeeze(), masked_token)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += (logits.argmax(dim=-1).squeeze()==masked_token).sum()/logits.shape[0]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        epoch_loss /= len(data_loader)\n",
    "        epoch_acc /= len(data_loader)\n",
    "        print(f\"epoch {epoch}: loss: {epoch_loss:.4f} acc: {round(epoch_acc.item()*100, 2)}%\")\n",
    "        losses.append(epoch_loss)\n",
    "        accs.append(epoch_acc)\n",
    "\n",
    "    return losses, accs            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_ds = IMDBMaskedBertDataset(path=\"../data/IMDB.csv\")\n",
    "data_loader = DataLoader(dataset=masked_ds,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         shuffle=True)\n",
    "bert_embedding = Embedding(vocab_size=VOCAB_SIZE,\n",
    "                           d_model=D_MODEL)\n",
    "bert_masked_lm = BERTMaskedLM(output_size=VOCAB_SIZE)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=bert_masked_lm.parameters(),\n",
    "                             lr=1e-3,\n",
    "                             betas=(0.9, 0.999),\n",
    "                             weight_decay=0.01)\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accs = train_masked_lm(bert=bert_masked_lm,\n",
    "                               bert_embedding=bert_embedding,\n",
    "                               data_loader=data_loader,\n",
    "                               loss_fn=loss_fn,\n",
    "                               optimizer=optimizer,\n",
    "                               epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
