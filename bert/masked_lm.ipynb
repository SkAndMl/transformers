{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CotKoDkKRFfD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "import math\n",
        "from typing import Tuple, List\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0, '..')\n",
        "\n",
        "from data_setup import IMDBMaskedBertDataset\n",
        "from model import BERTMaskedLM\n",
        "from scripts.scripts import Embedding, create_padding_mask\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "4aauDAx-k6rd"
      },
      "outputs": [],
      "source": [
        "URL = \"https://github.com/SK7here/Movie-Review-Sentiment-Analysis/raw/master/IMDB-Dataset.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "i9aWDP2ZRFfF",
        "outputId": "e6cd2b85-5d14-40a4-b30e-5f26ba9ef138"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>One of the other reviewers has mentioned that ...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I thought this was a wonderful way to spend ti...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Basically there's a family where a little boy ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49995</th>\n",
              "      <td>I thought this movie did a down right good job...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49996</th>\n",
              "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49997</th>\n",
              "      <td>I am a Catholic taught in parochial elementary...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49998</th>\n",
              "      <td>I'm going to have to disagree with the previou...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49999</th>\n",
              "      <td>No one expects the Star Trek movies to be high...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50000 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "0      One of the other reviewers has mentioned that ...  positive\n",
              "1      A wonderful little production. <br /><br />The...  positive\n",
              "2      I thought this was a wonderful way to spend ti...  positive\n",
              "3      Basically there's a family where a little boy ...  negative\n",
              "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
              "...                                                  ...       ...\n",
              "49995  I thought this movie did a down right good job...  positive\n",
              "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
              "49997  I am a Catholic taught in parochial elementary...  negative\n",
              "49998  I'm going to have to disagree with the previou...  negative\n",
              "49999  No one expects the Star Trek movies to be high...  negative\n",
              "\n",
              "[50000 rows x 2 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv(URL, encoding=\"utf-8\",\n",
        "                 on_bad_lines=\"skip\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "N785OCqbRFfI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"../config/base_config.json\", \"r\") as f:\n",
        "    config = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'batch_size': 64,\n",
              " 'block_size': 64,\n",
              " 'context_length': 512,\n",
              " 'd_model': 384,\n",
              " 'dropout': 0.1,\n",
              " 'epochs': 10,\n",
              " 'head_dim': 64,\n",
              " 'learning_rate': 3e-05,\n",
              " 'n_decoders': 6,\n",
              " 'n_encoders': 6,\n",
              " 'n_heads': 6}"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEWj6It0RFfJ"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "KRibTmUKRFfJ"
      },
      "outputs": [],
      "source": [
        "PAD_TOKEN = 0\n",
        "CLS_TOKEN = 1\n",
        "SEP_TOKEN = 2\n",
        "MASK_TOKEN = 3\n",
        "UNK_TOKEN = 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "config[\"epochs\"] = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "U3eQjkgJRFfJ"
      },
      "outputs": [],
      "source": [
        "def train_masked_lm(bert: BERTMaskedLM,\n",
        "                    data_loader: DataLoader,\n",
        "                    optimizer: torch.optim.Optimizer,\n",
        "                    device: torch.device=\"cpu\") -> Tuple[List[int], List[int]]:\n",
        "\n",
        "    losses = []\n",
        "    accs = []\n",
        "    bert = bert.to(device)\n",
        "    bert.train()\n",
        "    for epoch in tqdm(range(1, config[\"epochs\"]+1)):\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "        for batch, (sentence, masked_token, masked_token_idx) in tqdm(enumerate(data_loader)):\n",
        "            sentence = sentence.to(device)\n",
        "            masked_token = masked_token.to(device)\n",
        "            masked_token_idx = masked_token_idx.to(device)\n",
        "\n",
        "            logits, loss = bert(sentence, masked_token, masked_token_idx)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += (logits.argmax(dim=-1).squeeze()==masked_token).sum()/config[\"batch_size\"]\n",
        "\n",
        "            optimizer.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        epoch_loss /= len(data_loader)\n",
        "        epoch_acc /= len(data_loader)\n",
        "        print(f\"epoch {epoch}: loss: {epoch_loss:.4f} acc: {round(epoch_acc.item()*100, 2)}%\")\n",
        "        losses.append(epoch_loss)\n",
        "        accs.append(epoch_acc)\n",
        "\n",
        "    return losses, accs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "05qLpocm0wd3"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "  from torchinfo import summary\n",
        "except:\n",
        "  !pip install -q torchinfo\n",
        "  from torchinfo import summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "masked_ds = IMDBMaskedBertDataset(path=URL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab_size = len(masked_ds.vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "opiHcw4DT6Nr"
      },
      "outputs": [],
      "source": [
        "data_loader = DataLoader(dataset=masked_ds,\n",
        "                         batch_size=config[\"batch_size\"],\n",
        "                         shuffle=True)\n",
        "bert_embedding = Embedding(vocab_size=vocab_size,\n",
        "                           config=config)\n",
        "bert_masked_lm = BERTMaskedLM(config=config,\n",
        "                              vocab_size=vocab_size)\n",
        "optimizer = torch.optim.AdamW(params=bert_masked_lm.parameters(),\n",
        "                             lr=config[\"learning_rate\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F06Jazg0_Ok",
        "outputId": "60463a27-5578-483f-f7b0-2d0bd01f5d4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "===========================================================================\n",
              "Layer (type:depth-idx)                             Param #\n",
              "===========================================================================\n",
              "BERTMaskedLM                                       --\n",
              "â”œâ”€Embedding: 1-1                                   --\n",
              "â”‚    â””â”€Embedding: 2-1                              6,350,592\n",
              "â”‚    â””â”€Embedding: 2-2                              196,608\n",
              "â”‚    â””â”€Dropout: 2-3                                --\n",
              "â”œâ”€Encoder: 1-2                                     --\n",
              "â”‚    â””â”€ModuleList: 2-4                             --\n",
              "â”‚    â”‚    â””â”€EncoderBlock: 3-1                      1,774,464\n",
              "â”‚    â”‚    â””â”€EncoderBlock: 3-2                      1,774,464\n",
              "â”‚    â”‚    â””â”€EncoderBlock: 3-3                      1,774,464\n",
              "â”‚    â”‚    â””â”€EncoderBlock: 3-4                      1,774,464\n",
              "â”‚    â”‚    â””â”€EncoderBlock: 3-5                      1,774,464\n",
              "â”‚    â”‚    â””â”€EncoderBlock: 3-6                      1,774,464\n",
              "â”œâ”€Linear: 1-3                                      6,367,130\n",
              "===========================================================================\n",
              "Total params: 23,561,114\n",
              "Trainable params: 23,561,114\n",
              "Non-trainable params: 0\n",
              "==========================================================================="
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary(model=bert_masked_lm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "84dd9351b4c04ff1b2387d2c5780d521",
            "f85e0052dfc841dda2ebb04af9bf9d4b",
            "50ebe4f2a5fc4afbae5f7f66639827ad",
            "72802a55b58d414c9eb6880c1b2ce1f6",
            "01d1d1559d5a4d5bb62a9f5d2b376b4f",
            "d1b7b877704a48bb8e3c24d80f37a8ee",
            "c6ad5707a9204d3eac608a6196b79c70",
            "7341dc41b5014170bc97dce4b26c515e",
            "f5aa8281fcef4be6a93a63b79d556ee9",
            "7f0af985a3604c2cace070b11a84bb99",
            "99f2f9a1ec3f4806af1cdaff41b3e824"
          ]
        },
        "id": "lgydYt5jRFfJ",
        "outputId": "a2732ac7-0434-4a88-e1c3-d664808f1532"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "554363b2f389487fa3f98e42c71fd1f8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58c3dece035f4f6795b9899528066c6f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "0it [00:00, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "TypeError",
          "evalue": "BERTMaskedLM.forward() missing 1 required positional argument: 'masked_idx'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses, accs \u001b[39m=\u001b[39m train_masked_lm(bert\u001b[39m=\u001b[39;49mbert_masked_lm,\n\u001b[1;32m      2\u001b[0m                                data_loader\u001b[39m=\u001b[39;49mdata_loader,\n\u001b[1;32m      3\u001b[0m                                optimizer\u001b[39m=\u001b[39;49moptimizer)\n",
            "Cell \u001b[0;32mIn[34], line 18\u001b[0m, in \u001b[0;36mtrain_masked_lm\u001b[0;34m(bert, data_loader, optimizer, device)\u001b[0m\n\u001b[1;32m     15\u001b[0m masked_token \u001b[39m=\u001b[39m masked_token\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     16\u001b[0m masked_token_idx \u001b[39m=\u001b[39m masked_token_idx\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> 18\u001b[0m logits, loss \u001b[39m=\u001b[39m bert(sentence, masked_token, masked_token_idx)\n\u001b[1;32m     19\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     20\u001b[0m epoch_acc \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (logits\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39msqueeze()\u001b[39m==\u001b[39mmasked_token)\u001b[39m.\u001b[39msum()\u001b[39m/\u001b[39mconfig[\u001b[39m\"\u001b[39m\u001b[39mbatch_size\u001b[39m\u001b[39m\"\u001b[39m]\n",
            "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "\u001b[0;31mTypeError\u001b[0m: BERTMaskedLM.forward() missing 1 required positional argument: 'masked_idx'"
          ]
        }
      ],
      "source": [
        "losses, accs = train_masked_lm(bert=bert_masked_lm,\n",
        "                               data_loader=data_loader,\n",
        "                               optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scripts.scripts import Encoder\n",
        "from torch.nn import functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BERTMaskedLM(nn.Module):\n",
        "\n",
        "    PAD_TOKEN = 0\n",
        "    CLS_TOKEN = 1\n",
        "    SEP_TOKEN = 2\n",
        "    MASK_TOKEN = 3\n",
        "    UNK_TOKEN = 4\n",
        "\n",
        "    def __init__(self,\n",
        "                 config,\n",
        "                 vocab_size: int=1000) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = config[\"d_model\"]\n",
        "        self.n_heads = config[\"n_heads\"]\n",
        "        self.n_layers = config[\"n_encoders\"]\n",
        "        self.embedding = Embedding(config=config,\n",
        "                                   vocab_size=vocab_size)\n",
        "        self.bert = Encoder(config=config)\n",
        "        self.masked_lm = nn.Linear(in_features=self.d_model,\n",
        "                                      out_features=vocab_size)\n",
        "        \n",
        "    \n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"BERT(num_layers={self.n_layers}, d_model={self.d_model}, num_heads={self.n_heads})\"\n",
        "    \n",
        "    def __str__(self) -> str:\n",
        "        return f\"BERT(num_layers={self.n_layers}, d_model={self.d_model}, num_heads={self.n_heads})\"\n",
        "\n",
        "    def forward(self,\n",
        "                x: torch.Tensor,\n",
        "                masked_tokens: torch.Tensor,\n",
        "                masked_idx: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \n",
        "        mask = create_padding_mask(batch=x, padding_idx=self.PAD_TOKEN)\n",
        "\n",
        "        # x -> [B, S]\n",
        "        x = self.embedding(x) # [B, S, D_MODEL]\n",
        "        x = self.bert(x, mask) # [B, S, D_MODEL]\n",
        "        x = x[range(len(masked_idx)), masked_idx, :].squeeze() # B, D_MODEL\n",
        "        logits = self.masked_lm(x) # B, VOCAB_SIZE\n",
        "\n",
        "        loss = F.cross_entropy(logits, masked_tokens.squeeze())\n",
        "\n",
        "        return logits, loss\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Boolean value of Tensor with more than one value is ambiguous",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[47], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m bert \u001b[39m=\u001b[39m BERTMaskedLM(config\u001b[39m=\u001b[39mconfig,\n\u001b[1;32m      2\u001b[0m                     vocab_size\u001b[39m=\u001b[39mvocab_size)\n\u001b[1;32m      3\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[0;32m----> 4\u001b[0m     logits, losses \u001b[39m=\u001b[39m bert\u001b[39m.\u001b[39;49mforward(torch\u001b[39m.\u001b[39;49mrandint(low\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, high\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, size\u001b[39m=\u001b[39;49m(config[\u001b[39m\"\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m], \u001b[39m5\u001b[39;49m)),\n\u001b[1;32m      5\u001b[0m                                   masked_tokens\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mrandint(low\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, high\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, size\u001b[39m=\u001b[39;49m(config[\u001b[39m\"\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m],)),\n\u001b[1;32m      6\u001b[0m                                   masked_idx\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mrandint(low\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, high\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, size\u001b[39m=\u001b[39;49m(config[\u001b[39m\"\u001b[39;49m\u001b[39mbatch_size\u001b[39;49m\u001b[39m\"\u001b[39;49m],)))\n",
            "Cell \u001b[0;32mIn[46], line 39\u001b[0m, in \u001b[0;36mBERTMaskedLM.forward\u001b[0;34m(self, x, masked_tokens, masked_idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39m# x -> [B, S]\u001b[39;00m\n\u001b[1;32m     38\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding(x) \u001b[39m# [B, S, D_MODEL]\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(x, mask) \u001b[39m# [B, S, D_MODEL]\u001b[39;00m\n\u001b[1;32m     40\u001b[0m x \u001b[39m=\u001b[39m x[\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(masked_idx)), masked_idx, :]\u001b[39m.\u001b[39msqueeze() \u001b[39m# B, D_MODEL\u001b[39;00m\n\u001b[1;32m     41\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmasked_lm(x) \u001b[39m# B, VOCAB_SIZE\u001b[39;00m\n",
            "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Desktop/transformers/bert/../scripts/scripts.py:198\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    194\u001b[0m             x: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    195\u001b[0m             mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    197\u001b[0m     \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m--> 198\u001b[0m         x \u001b[39m=\u001b[39m block(x, mask)\n\u001b[1;32m    199\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Desktop/transformers/bert/../scripts/scripts.py:181\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    178\u001b[0m             x: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    179\u001b[0m             mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 181\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmha(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(x), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(x), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mln1(x), mask)\n\u001b[1;32m    182\u001b[0m     x \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mff(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln2(x))\n\u001b[1;32m    183\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
            "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Desktop/transformers/bert/../scripts/scripts.py:141\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \n\u001b[1;32m    136\u001b[0m             query: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    137\u001b[0m             key: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    138\u001b[0m             value: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    139\u001b[0m             mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 141\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h(query, key, value, mask) \u001b[39mfor\u001b[39;49;00m h \u001b[39min\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msa_heads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    142\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(out)\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(out)\n",
            "File \u001b[0;32m~/Desktop/transformers/bert/../scripts/scripts.py:141\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \n\u001b[1;32m    136\u001b[0m             query: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    137\u001b[0m             key: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    138\u001b[0m             value: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    139\u001b[0m             mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 141\u001b[0m     out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([h(query, key, value, mask) \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msa_heads], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    142\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(out)\n\u001b[1;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(out)\n",
            "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/Desktop/transformers/bert/../scripts/scripts.py:117\u001b[0m, in \u001b[0;36mAttentionHead.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m    114\u001b[0m v \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(value) \u001b[39m# B, K, HEAD_DIM\u001b[39;00m\n\u001b[1;32m    116\u001b[0m weights \u001b[39m=\u001b[39m q \u001b[39m@\u001b[39m k\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m) \u001b[39m# B, Q, K\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m \u001b[39mif\u001b[39;00m mask:\n\u001b[1;32m    118\u001b[0m     weights \u001b[39m=\u001b[39m weights\u001b[39m.\u001b[39mmasked_fill(mask\u001b[39m==\u001b[39m\u001b[39m0\u001b[39m, value\u001b[39m=\u001b[39m\u001b[39mfloat\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-inf\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    119\u001b[0m weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(weights\u001b[39m/\u001b[39mmath\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim))\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
          ]
        }
      ],
      "source": [
        "bert = BERTMaskedLM(config=config,\n",
        "                    vocab_size=vocab_size)\n",
        "with torch.inference_mode():\n",
        "    logits, losses = bert.forward(torch.randint(low=0, high=10, size=(config[\"batch_size\"], 5)),\n",
        "                                  masked_tokens=torch.randint(low=0, high=5, size=(config[\"batch_size\"],)),\n",
        "                                  masked_idx=torch.randint(low=0, high=10, size=(config[\"batch_size\"],)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01d1d1559d5a4d5bb62a9f5d2b376b4f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50ebe4f2a5fc4afbae5f7f66639827ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7341dc41b5014170bc97dce4b26c515e",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f5aa8281fcef4be6a93a63b79d556ee9",
            "value": 10
          }
        },
        "72802a55b58d414c9eb6880c1b2ce1f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f0af985a3604c2cace070b11a84bb99",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_99f2f9a1ec3f4806af1cdaff41b3e824",
            "value": " 10/10 [44:49&lt;00:00, 268.53s/it]"
          }
        },
        "7341dc41b5014170bc97dce4b26c515e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f0af985a3604c2cace070b11a84bb99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84dd9351b4c04ff1b2387d2c5780d521": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f85e0052dfc841dda2ebb04af9bf9d4b",
              "IPY_MODEL_50ebe4f2a5fc4afbae5f7f66639827ad",
              "IPY_MODEL_72802a55b58d414c9eb6880c1b2ce1f6"
            ],
            "layout": "IPY_MODEL_01d1d1559d5a4d5bb62a9f5d2b376b4f"
          }
        },
        "99f2f9a1ec3f4806af1cdaff41b3e824": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6ad5707a9204d3eac608a6196b79c70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1b7b877704a48bb8e3c24d80f37a8ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5aa8281fcef4be6a93a63b79d556ee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f85e0052dfc841dda2ebb04af9bf9d4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1b7b877704a48bb8e3c24d80f37a8ee",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c6ad5707a9204d3eac608a6196b79c70",
            "value": "100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
